{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n# Estimating the Effect of Schooling on Wages: Intrumental Variables Application\n\n### Summary of Contents:\n1. [Introduction](#intro)\n2. [NLSYM Dataset](#data)\n3. [A Gentle Start: The Naive Approach](#naive)\n4. [Using Instrumental Variables: 2SLS](#2sls)\n5. [Bonus: Deep Instrumental Variables](#deepiv)\n\n**Important:** This notebook provides some room for experimentation in solving this problem. If you are looking for the end-to-end solution, look for the same file name with the \"Complete\" suffix added."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1. Introduction <a class=\"anchor\" id=\"intro\"></a>\n\nTo measure true causal effects of a treatment $T$ on an outcome $Y$ from observational data, we need to record all features $X$ that might influence both $T$ and $Y$. These $X$'s are called confounders. \n\nWhen some confounders are not recorded in the data, we might get biased estimates of the treatment effect. Here is an example:\n* Children of high-income parents might attain higher levels of education (e.g. college) since they can afford it\n* Children of high-income parents might also obtain better paying jobs due to parents' connections and knowledge\n* At first sight, it might appear as if education has an effect on income, when in fact this could be fully explained by family background\n\nThere are several reasons for not recording all possible confounders, such as incomplete data or a confounder that is difficult to quantify (e.g. parental involvement). However, not all is lost! In cases such as these, we can use instrumental variables $Z$, features that affect the outcome only through their effect on the treatment. \n\nIn this notebook, we use a real-world problem to show how treatment effects can be extracted with the help of instrumental variables. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 2. NLSYM Dataset <a class=\"anchor\" id=\"data\"></a>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The **causal impact of schooling on wages** had been studied at length. Although it is generally agreed that there is a positive impact, it is difficult to measure this effect precisely. The core problem is that education levels are not assigned at random in the population and we cannot record all possible confounders. (Think about the value parents assign to education. How would you quantify how valuable parents think their children's education is?). \n\n<img src=\"https://straubroland.files.wordpress.com/2010/12/education_technology-resized-600.png\" width=400px/>\n\nTo get around this issue, we can use **proximity to a 4-year college** as an instrumental variable. Having a college nearby can allow individuals (especially low-income ones) to complete more years of education. Hence, if there was a positive treatment effect, we would expect these individuals to have higher wages on average. Note that college proximity is a valid IV since it does not affect wages directly.  \n\nWe use data from the National Longitudinal Survey of Young Men (NLSYM, 1966) to estimate the average treatment effect (ATE) of education on wages (see also [Card, 1999](https://www.nber.org/papers/w4483)). The NLSYM data contains entries from men ages 14-24 that were interviewed in 1966 and again in 1976. \n\nThe dataset contains the following variables:\n* $Y$ (outcome): wages (log)\n* $T$ (treatment): years of schooling\n* $Z$ (IV): proximity to a 4-year college (binary)\n* $X$ (heterogeneity): e.g. parental education\n* $W$ (controls): e.g. family composition, location, etc.\n\nThe world can then be modelled as:\n$$\n\\begin{align}\nY & = \\theta(X) \\cdot T + f(W) + \\epsilon\\\\\nT & = g(Z, W) + \\eta\n\\end{align}\n$$\nwhere $\\epsilon, \\eta$ are uncorrelated error terms."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Python imports\nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\n# EconML imports\nfrom econml.dml import DMLCateEstimator\nfrom econml.two_stage_least_squares import NonparametricTwoStageLeastSquares\nfrom econml.deepiv import DeepIVEstimator\n\n%matplotlib inline",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Data processing\ndf = pd.read_csv(\"data/card.csv\", dtype=float)\n# Filter out individuals with low education levels (outliers)\ndata_filter = df['educ'].values >= 6\n# Define some variables\nT = df['educ'].values[data_filter]\nZ = df['nearc4'].values[data_filter]\nY = df['lwage'].values[data_filter]\n\n# Impute missing values with mean, add dummy columns\n# Filter outliers (interviewees with less than 6 years of education)\nX_df = df[['exper', 'expersq']].copy()\nX_df['fatheduc'] = df['fatheduc'].fillna(value=df['fatheduc'].mean())\nX_df['fatheduc_nan'] = df['fatheduc'].isnull() * 1\nX_df['motheduc'] = df['motheduc'].fillna(value=df['motheduc'].mean())\nX_df['motheduc_nan'] = df['motheduc'].isnull() * 1\nX_df[['momdad14', 'sinmom14', 'reg661', 'reg662',\n        'reg663', 'reg664', 'reg665', 'reg666', 'reg667', 'reg668', 'reg669', 'south66']] = df[['momdad14', 'sinmom14', \n        'reg661', 'reg662','reg663', 'reg664', 'reg665', 'reg666', 'reg667', 'reg668', 'reg669', 'south66']]\nX_df[['black', 'smsa', 'south', 'smsa66']] = df[['black', 'smsa', 'south', 'smsa66']]\ncolumns_to_scale = ['fatheduc', 'motheduc', 'exper', 'expersq']\n# Scale continuous variables\nscaler = StandardScaler()\nX_df[columns_to_scale] = scaler.fit_transform(X_df[columns_to_scale])\nX = X_df.values[data_filter]",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Explore data\nX_df.head()",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>exper</th>\n      <th>expersq</th>\n      <th>fatheduc</th>\n      <th>fatheduc_nan</th>\n      <th>motheduc</th>\n      <th>motheduc_nan</th>\n      <th>momdad14</th>\n      <th>sinmom14</th>\n      <th>reg661</th>\n      <th>reg662</th>\n      <th>...</th>\n      <th>reg665</th>\n      <th>reg666</th>\n      <th>reg667</th>\n      <th>reg668</th>\n      <th>reg669</th>\n      <th>south66</th>\n      <th>black</th>\n      <th>smsa</th>\n      <th>south</th>\n      <th>smsa66</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.725159</td>\n      <td>1.896133</td>\n      <td>5.439188e-16</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.034739</td>\n      <td>-0.172321</td>\n      <td>-6.134540e-01</td>\n      <td>0</td>\n      <td>-0.786159</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.725159</td>\n      <td>1.896133</td>\n      <td>1.223740e+00</td>\n      <td>0</td>\n      <td>0.553046</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.276228</td>\n      <td>0.052254</td>\n      <td>3.051432e-01</td>\n      <td>0</td>\n      <td>0.553046</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.725159</td>\n      <td>1.896133</td>\n      <td>-6.134540e-01</td>\n      <td>0</td>\n      <td>-1.120960</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>",
            "text/plain": "      exper   expersq      fatheduc  fatheduc_nan  motheduc  motheduc_nan  \\\n0  1.725159  1.896133  5.439188e-16             1  0.000000             1   \n1  0.034739 -0.172321 -6.134540e-01             0 -0.786159             0   \n2  1.725159  1.896133  1.223740e+00             0  0.553046             0   \n3  0.276228  0.052254  3.051432e-01             0  0.553046             0   \n4  1.725159  1.896133 -6.134540e-01             0 -1.120960             0   \n\n   momdad14  sinmom14  reg661  reg662   ...    reg665  reg666  reg667  reg668  \\\n0       1.0       0.0     1.0     0.0   ...       0.0     0.0     0.0     0.0   \n1       1.0       0.0     1.0     0.0   ...       0.0     0.0     0.0     0.0   \n2       1.0       0.0     1.0     0.0   ...       0.0     0.0     0.0     0.0   \n3       1.0       0.0     0.0     1.0   ...       0.0     0.0     0.0     0.0   \n4       1.0       0.0     0.0     1.0   ...       0.0     0.0     0.0     0.0   \n\n   reg669  south66  black  smsa  south  smsa66  \n0     0.0      0.0    1.0   1.0    0.0     1.0  \n1     0.0      0.0    0.0   1.0    0.0     1.0  \n2     0.0      0.0    0.0   1.0    0.0     1.0  \n3     0.0      0.0    0.0   1.0    0.0     1.0  \n4     0.0      0.0    0.0   1.0    0.0     1.0  \n\n[5 rows x 22 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 3. A Gentle Start: The Naive Approach <a class=\"anchor\" id=\"naive\"></a>\n\nLet's assume we know nothing about instrumental variables and we want to measure the treatment effect of schooling on wages. We can apply an IV-free method like Double Machine Learning (DML) to do this and extract a treatment effect. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### DIY Section!\n# Define a DML Estimator\ndml_est = None",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fit the DML estimator and calculate the heterogeneous treatment effect\ndml_te = None\n# Uncoment after you calculated dml_te\n\"\"\"\ndml_ate = dml_te.mean()\n\"\"\"",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "'\\ndml_ate = dml_te.mean()\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Uncomment after you calculated the dml_ate\n\"\"\"\nprint(\"Average treatment effect: {0:.3f}\".format(dml_ate))\n\"\"\"",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'\\nprint(\"Average treatment effect: {0:.3f}\".format(dml_ate))\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This treatment effect is smaller than other values obtained in literature via IV. Why could that be? \n\nBecause DML (like all IV-free methods) assumes that the residual errors are uncorrelated (i.e. $Y - \\hat{Y}$ is uncorrelated with $T - \\hat{T}$). Let's test this assumption:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### DIY Section\n# Calculate the residuals Y_res = Y - Y_pred and T_res = T - T_pred\n# Bonus: Split data in 2 parts, train on one part and predict on the other to avoid over-fitting \nT_res, Y_res = np.zeros(T.shape[0]), np.zeros(Y.shape[0])",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Uncomment this after you calculated T_res and Y_res\n\"\"\"\nplt.scatter(T_res, Y_res)\nplt.show()\n\"\"\"",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "'\\nplt.scatter(T_res, Y_res)\\nplt.show()\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Uncomment this after you calculated T_res and Y_res\n\"\"\"\ncorr_coeficient = pearsonr(T_res, Y_res)[0]\nprint(\"Correlation coefficient between T and Y errors: {0:.2f}\".format(corr_coeficient))\n\"\"\"",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "'\\ncorr_coeficient = pearsonr(T_res, Y_res)[0]\\nprint(\"Correlation coefficient between T and Y errors: {0:.2f}\".format(corr_coeficient))\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The correlation coefficient between the residuals is quite large, which means that there is some unobserved variables that affect both $T$ and $Y$. To get an accurate estimate in this case, we need to use IVs. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 4. Using Intrumental Variables: 2SLS <a class=\"anchor\" id=\"2sls\"></a>\n\nTwo stage least square regression procedure (2SLS):\n1. Fit a model $T \\sim W, Z$\n2. Fit a linear model $Y \\sim \\hat{T}$ where $\\hat{T}$ is the prediction of the model in step 1.\nThe coefficient from 2. above is the average treatment effect.\n\nIf interested in heterogeneous treatment effects, fit a model $Y \\sim \\hat{T}\\otimes h(X)$, where $h(X)$ is a chosen featurization of the treatment effect. \n\nFor more information, see the `econml` [documentation](https://econml.azurewebsites.net)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# For average treatment effects, X is a column of 1s\nW = X\nZ = Z.reshape(-1, 1)\nT = T.reshape(-1, 1)\nX_ate = np.ones_like(Z)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# We apply 2SLS from the EconML library\ntwo_sls_est = NonparametricTwoStageLeastSquares(\n    t_featurizer=PolynomialFeatures(degree=1, include_bias=False),\n    x_featurizer=PolynomialFeatures(degree=1, include_bias=False),\n    z_featurizer=PolynomialFeatures(degree=1, include_bias=False),\n    dt_featurizer=None) # dt_featurizer only matters for marginal_effect",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "two_sls_est.fit(Y, T, X_ate, W, Z)\ntwo_sls_ate = two_sls_est.effect(np.ones((1,1)))[0]",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Average treatment effect: {0:.3f}\".format(two_sls_ate))",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Average treatment effect: 0.133\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 5. Bonus: Deep Instrumental Variables <a class=\"anchor\" id=\"deepiv\"></a>\n\nFor very flexible, but fully non-parametric IV methods, you can use neural networks for the two models in 2SLS and a mixture of gaussians for the featurizer $h(X)$. In `econml`, this method is called DeepIV. \n\nThe NLSYM dataset is small (on neural net scale) so applying DeepIV is a bit of a stretch. Nevertheless, we apply DeepIV the NLSYM data as an example. You should not read too much into the results. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define treatment model, T ~ X, Z\ntreatment_model = keras.Sequential([keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1] + 1,)),\n                                    keras.layers.Dropout(rate=0.17),\n                                    keras.layers.Dense(32, activation='relu'),\n                                    keras.layers.Dropout(rate=0.17),\n                                    keras.layers.Dense(1)])\n# Define outcome model, Y ~ T_hat, X\nresponse_model = keras.Sequential([keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1] + 1,)),\n                                   keras.layers.Dropout(rate=0.17),\n                                   keras.layers.Dense(32, activation='relu'),\n                                   keras.layers.Dropout(rate=0.17),\n                                   keras.layers.Dense(1)])",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "keras_fit_options = { \"epochs\": 30,\n                      \"validation_split\": 0.3,\n                      \"callbacks\": [keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)]}\n\ndeepIvEst = DeepIVEstimator(n_components = 10, # number of gaussians in our mixture density network\n                            m = lambda z, x : treatment_model(keras.layers.concatenate([z,x])), # treatment model\n                            h = lambda t, x : response_model(keras.layers.concatenate([t,x])),  # response model\n                            n_samples = 1, # number of samples to use to estimate the response\n                            use_upper_bound_loss = False, # whether to use an approximation to the true loss\n                            n_gradient_samples = 1, # number of samples to use in second estimate of the response (to make loss estimate unbiased)\n                            optimizer='adam', # Keras optimizer to use for training - see https://keras.io/optimizers/ \n                            first_stage_options = keras_fit_options, # options for training treatment model\n                            second_stage_options = keras_fit_options) # options for training response model",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "deepIvEst.fit(Y, T, X, Z)\ndeepIv_effect = deepIvEst.effect(X)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 2093 samples, validate on 898 samples\nEpoch 1/30\n2093/2093 [==============================] - 4s 2ms/step - loss: 30.7306 - val_loss: 6.4615\nEpoch 2/30\n2093/2093 [==============================] - 0s 198us/step - loss: 5.1357 - val_loss: 3.4685\nEpoch 3/30\n2093/2093 [==============================] - 0s 151us/step - loss: 3.5259 - val_loss: 3.1621\nEpoch 4/30\n2093/2093 [==============================] - 0s 170us/step - loss: 3.3261 - val_loss: 3.0245\nEpoch 5/30\n2093/2093 [==============================] - 0s 183us/step - loss: 3.1779 - val_loss: 2.9176\nEpoch 6/30\n2093/2093 [==============================] - 0s 168us/step - loss: 3.0697 - val_loss: 2.8208\nEpoch 7/30\n2093/2093 [==============================] - 0s 161us/step - loss: 2.9555 - val_loss: 2.7179\nEpoch 8/30\n2093/2093 [==============================] - 0s 163us/step - loss: 2.8241 - val_loss: 2.5836\nEpoch 9/30\n2093/2093 [==============================] - 0s 155us/step - loss: 2.6044 - val_loss: 2.3252\nEpoch 10/30\n2093/2093 [==============================] - 0s 191us/step - loss: 2.2062 - val_loss: 1.7261\nEpoch 11/30\n2093/2093 [==============================] - 1s 696us/step - loss: 1.7024 - val_loss: 1.4514\nEpoch 12/30\n2093/2093 [==============================] - 1s 344us/step - loss: 1.6211 - val_loss: 1.3976\nEpoch 13/30\n2093/2093 [==============================] - 0s 151us/step - loss: 1.5679 - val_loss: 1.3887\nEpoch 14/30\n2093/2093 [==============================] - 0s 136us/step - loss: 1.5611 - val_loss: 1.3335\nEpoch 15/30\n2093/2093 [==============================] - 0s 120us/step - loss: 1.5211 - val_loss: 1.3349\nEpoch 16/30\n2093/2093 [==============================] - 0s 175us/step - loss: 1.5480 - val_loss: 1.3586\nTrain on 2093 samples, validate on 898 samples\nEpoch 1/30\n2093/2093 [==============================] - 6s 3ms/step - loss: 9.7604 - val_loss: 4.0337\nEpoch 2/30\n2093/2093 [==============================] - 0s 210us/step - loss: 5.7451 - val_loss: 5.7511\nEpoch 3/30\n2093/2093 [==============================] - 1s 242us/step - loss: 6.0714 - val_loss: 2.3616\nEpoch 4/30\n2093/2093 [==============================] - 1s 241us/step - loss: 5.4732 - val_loss: 2.9394\nEpoch 5/30\n2093/2093 [==============================] - 0s 195us/step - loss: 5.5349 - val_loss: 3.6689\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Average treatment effect: {0:.3f}\".format(deepIv_effect.mean()))",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Average treatment effect: 0.244\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Heterogeneity of treatment effects\nplt.hist(deepIv_effect)\nplt.show()",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}