
@article{lipton_mythos_2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	urldate = {2020-03-08},
	journal = {arXiv:1606.03490 [cs, stat]},
	author = {Lipton, Zachary C.},
	month = mar,
	year = {2017},
	note = {arXiv: 1606.03490},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: presented at 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY},
	file = {3236386.3241340.pdf:C\:\\Users\\13762012\\Zotero\\storage\\3BK4MMPE\\3236386.3241340.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\DUXP935F\\1606.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\ILE5E4PW\\Lipton - 2017 - The Mythos of Model Interpretability.pdf:application/pdf}
}

@article{hara_making_2017,
	title = {Making {Tree} {Ensembles} {Interpretable}: {A} {Bayesian} {Model} {Selection} {Approach}},
	shorttitle = {Making {Tree} {Ensembles} {Interpretable}},
	url = {http://arxiv.org/abs/1606.09066},
	abstract = {Tree ensembles, such as random forests and boosted trees, are renowned for their high prediction performance. However, their interpretability is critically limited due to the enormous complexity. In this study, we present a method to make a complex tree ensemble interpretable by simplifying the model. Specifically, we formalize the simplification of tree ensembles as a model selection problem. Given a complex tree ensemble, we aim at obtaining the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm that optimizes the simplified model while maintaining the prediction performance. Our numerical experiments on several datasets showed that complicated tree ensembles were reasonably approximated as interpretable.},
	urldate = {2020-03-08},
	journal = {arXiv:1606.09066 [stat]},
	author = {Hara, Satoshi and Hayashi, Kohei},
	month = feb,
	year = {2017},
	note = {arXiv: 1606.09066},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: 21 pages},
	file = {arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\3M5RVTSY\\1606.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\56JG4TT6\\Hara and Hayashi - 2017 - Making Tree Ensembles Interpretable A Bayesian Mo.pdf:application/pdf}
}

@incollection{guo_explaining_2018,
	title = {Explaining {Deep} {Learning} {Models} – {A} {Bayesian} {Non}-parametric {Approach}},
	url = {http://papers.nips.cc/paper/7703-explaining-deep-learning-models-a-bayesian-non-parametric-approach.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {4514--4524},
	file = {Guo et al. - 2018 - Explaining Deep Learning Models – A Bayesian Non-p.pdf:C\:\\Users\\13762012\\Zotero\\storage\\W66GNK9I\\Guo et al. - 2018 - Explaining Deep Learning Models – A Bayesian Non-p.pdf:application/pdf}
}

@article{carvalho_machine_2019,
	title = {Machine {Learning} {Interpretability}: {A} {Survey} on {Methods} and {Metrics}},
	volume = {8},
	issn = {2079-9292},
	shorttitle = {Machine {Learning} {Interpretability}},
	url = {https://www.mdpi.com/2079-9292/8/8/832},
	doi = {10.3390/electronics8080832},
	abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
	language = {en},
	number = {8},
	urldate = {2020-03-07},
	journal = {Electronics},
	author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
	month = jul,
	year = {2019},
	pages = {832},
	file = {electronics-08-00832.pdf:C\:\\Users\\13762012\\Zotero\\storage\\3HF5UKT6\\electronics-08-00832.pdf:application/pdf;Full Text:C\:\\Users\\13762012\\Zotero\\storage\\U2YKFVIC\\Carvalho et al. - 2019 - Machine Learning Interpretability A Survey on Met.pdf:application/pdf}
}

@article{vedantam_probabilistic_2019,
	title = {Probabilistic {Neural}-symbolic {Models} for {Interpretable} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1902.07864},
	abstract = {We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring lesser number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed.},
	urldate = {2020-03-07},
	journal = {arXiv:1902.07864 [cs, stat]},
	author = {Vedantam, Ramakrishna and Desai, Karan and Lee, Stefan and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
	month = jun,
	year = {2019},
	note = {arXiv: 1902.07864},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML 2019 Camera Ready + Appendix},
	file = {arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\BMRZFSPE\\1902.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\L5YXDH6R\\Vedantam et al. - 2019 - Probabilistic Neural-symbolic Models for Interpret.pdf:application/pdf}
}

@inproceedings{balog_transparent_2019,
	address = {Paris, France},
	title = {Transparent, {Scrutable} and {Explainable} {User} {Models} for {Personalized} {Recommendation}},
	isbn = {978-1-4503-6172-9},
	url = {http://dl.acm.org/citation.cfm?doid=3331184.3331211},
	doi = {10.1145/3331184.3331211},
	language = {en},
	urldate = {2020-03-07},
	booktitle = {Proceedings of the 42nd {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}  - {SIGIR}'19},
	publisher = {ACM Press},
	author = {Balog, Krisztian and Radlinski, Filip and Arakelyan, Shushan},
	year = {2019},
	pages = {265--274},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\EPMNV7MM\\Balog et al. - 2019 - Transparent, Scrutable and Explainable User Models.pdf:application/pdf}
}

@article{erion_learning_2019,
	title = {Learning {Explainable} {Models} {Using} {Attribution} {Priors}},
	url = {http://arxiv.org/abs/1906.10670},
	abstract = {Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by constraining the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. We propose connecting these topics with attribution priors (https://github.com/suinleelab/attributionpriors), which allow humans to use the common language of attributions to enforce prior expectations about a model's behavior during training. We develop a differentiable axiomatic feature attribution method called expected gradients and show how to directly regularize these attributions during training. We demonstrate the broad applicability of attribution priors (\${\textbackslash}Omega\$) by presenting three distinct examples that regularize models to behave more intuitively in three different domains: 1) on image data, \${\textbackslash}Omega\_\{{\textbackslash}textrm\{pixel\}\}\$ encourages models to have piecewise smooth attribution maps; 2) on gene expression data, \${\textbackslash}Omega\_\{{\textbackslash}textrm\{graph\}\}\$ encourages models to treat functionally related genes similarly; 3) on a health care dataset, \${\textbackslash}Omega\_\{{\textbackslash}textrm\{sparse\}\}\$ encourages models to rely on fewer features. In all three domains, attribution priors produce models with more intuitive behavior and better generalization performance by encoding constraints that would otherwise be very difficult to encode using standard model priors.},
	urldate = {2020-03-07},
	journal = {arXiv:1906.10670 [cs, stat]},
	author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\7HWVECFG\\Erion et al. - 2019 - Learning Explainable Models Using Attribution Prio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\BCE6UQWD\\1906.html:text/html}
}

@article{afrabandpey_making_2019,
	title = {Making {Bayesian} {Predictive} {Models} {Interpretable}: {A} {Decision} {Theoretic} {Approach}},
	shorttitle = {Making {Bayesian} {Predictive} {Models} {Interpretable}},
	url = {http://arxiv.org/abs/1910.09358},
	abstract = {A salient approach to interpretable machine learning is to restrict modeling to simple and hence understandable models. In the Bayesian framework, this can be pursued by restricting the model structure and prior to favor interpretable models. Fundamentally, however, interpretability is about users' preferences, not the data generation mechanism: it is more natural to formulate interpretability as a utility function. In this work, we propose an interpretability utility, which explicates the trade-off between explanation fidelity and interpretability in the Bayesian framework. The method consists of two steps. First, a reference model, possibly a black-box Bayesian predictive model compromising no accuracy, is constructed and fitted to the training data. Second, a proxy model from an interpretable model family that best mimics the predictive behaviour of the reference model is found by optimizing the interpretability utility function. The approach is model agnostic - neither the interpretable model nor the reference model are restricted to be from a certain class of models - and the optimization problem can be solved using standard tools in the chosen model family. Through experiments on real-word data sets using decision trees as interpretable models and Bayesian additive regression models as reference models, we show that for the same level of interpretability, our approach generates more accurate models than the earlier alternative of restricting the prior. We also propose a systematic way to measure stabilities of interpretabile models constructed by different interpretability approaches and show that our proposed approach generates more stable models.},
	urldate = {2020-03-07},
	journal = {arXiv:1910.09358 [cs, stat]},
	author = {Afrabandpey, Homayun and Peltola, Tomi and Piironen, Juho and Vehtari, Aki and Kaski, Samuel},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.09358},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\N6G7JJX4\\1910.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\URP8WBV2\\Afrabandpey et al. - 2019 - Making Bayesian Predictive Models Interpretable A.pdf:application/pdf}
}

@inproceedings{yan_groupinn_2019,
	address = {Anchorage, AK, USA},
	title = {{GroupINN}: {Grouping}-based {Interpretable} {Neural} {Network} for {Classification} of {Limited}, {Noisy} {Brain} {Data}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {{GroupINN}},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330921},
	doi = {10.1145/3292500.3330921},
	language = {en},
	urldate = {2020-03-07},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}  - {KDD} '19},
	publisher = {ACM Press},
	author = {Yan, Yujun and Zhu, Jiong and Duda, Marlena and Solarz, Eric and Sripada, Chandra and Koutra, Danai},
	year = {2019},
	pages = {772--782},
	file = {Yan et al. - 2019 - GroupINN Grouping-based Interpretable Neural Netw.pdf:C\:\\Users\\13762012\\Zotero\\storage\\7HVYGBHU\\Yan et al. - 2019 - GroupINN Grouping-based Interpretable Neural Netw.pdf:application/pdf}
}

@inproceedings{yoshida_learning_2019,
	address = {Anchorage, AK, USA},
	title = {Learning {Interpretable} {Metric} between {Graphs}: {Convex} {Formulation} and {Computation} with {Graph} {Mining}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Learning {Interpretable} {Metric} between {Graphs}},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330845},
	doi = {10.1145/3292500.3330845},
	language = {en},
	urldate = {2020-03-07},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}  - {KDD} '19},
	publisher = {ACM Press},
	author = {Yoshida, Tomoki and Takeuchi, Ichiro and Karasuyama, Masayuki},
	year = {2019},
	pages = {1026--1036},
	file = {Yoshida et al. - 2019 - Learning Interpretable Metric between Graphs Conv.pdf:C\:\\Users\\13762012\\Zotero\\storage\\WHWT3LNY\\Yoshida et al. - 2019 - Learning Interpretable Metric between Graphs Conv.pdf:application/pdf}
}

@inproceedings{ming_interpretable_2019,
	address = {Anchorage, AK, USA},
	title = {Interpretable and {Steerable} {Sequence} {Learning} via {Prototypes}},
	isbn = {978-1-4503-6201-6},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330908},
	doi = {10.1145/3292500.3330908},
	language = {en},
	urldate = {2020-03-07},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}  - {KDD} '19},
	publisher = {ACM Press},
	author = {Ming, Yao and Xu, Panpan and Qu, Huamin and Ren, Liu},
	year = {2019},
	pages = {903--913},
	file = {Ming et al. - 2019 - Interpretable and Steerable Sequence Learning via .pdf:C\:\\Users\\13762012\\Zotero\\storage\\FA39HQI9\\Ming et al. - 2019 - Interpretable and Steerable Sequence Learning via .pdf:application/pdf}
}

@inproceedings{jia_improving_2019,
	address = {Anchorage, AK, USA},
	title = {Improving the {Quality} of {Explanations} with {Local} {Embedding} {Perturbations}},
	isbn = {978-1-4503-6201-6},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330930},
	doi = {10.1145/3292500.3330930},
	language = {en},
	urldate = {2020-03-06},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}  - {KDD} '19},
	publisher = {ACM Press},
	author = {Jia, Yunzhe and Bailey, James and Ramamohanarao, Kotagiri and Leckie, Christopher and Houle, Michael E.},
	year = {2019},
	pages = {875--884},
	file = {Jia et al. - 2019 - Improving the Quality of Explanations with Local E.pdf:C\:\\Users\\13762012\\Zotero\\storage\\WPBTDPVW\\Jia et al. - 2019 - Improving the Quality of Explanations with Local E.pdf:application/pdf}
}

@article{guo_survey_2019,
	title = {A {Survey} of {Learning} {Causality} with {Data}: {Problems} and {Methods}},
	shorttitle = {A {Survey} of {Learning} {Causality} with {Data}},
	url = {http://arxiv.org/abs/1809.09337},
	abstract = {The era of big data provides researchers with convenient access to copious data. However, people often have little knowledge about it. The increasing prevalence of big data is challenging the traditional methods of learning causality because they are developed for the cases with limited amount of data and solid prior causal knowledge. This survey aims to close the gap between big data and learning causality with a comprehensive and structured review of traditional and frontier methods and a discussion about some open problems of learning causality. We begin with preliminaries of learning causality. Then we categorize and revisit methods of learning causality for the typical problems and data types. After that, we discuss the connections between learning causality and machine learning. At the end, some open problems are presented to show the great potential of learning causality with data.},
	urldate = {2020-03-06},
	journal = {arXiv:1809.09337 [cs, stat]},
	author = {Guo, Ruocheng and Cheng, Lu and Li, Jundong and Hahn, P. Richard and Liu, Huan},
	month = apr,
	year = {2019},
	note = {arXiv: 1809.09337},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: 35 pages, under review},
	file = {arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\IFCPZGFE\\1809.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\JBMLPGM7\\Guo et al. - 2019 - A Survey of Learning Causality with Data Problems.pdf:application/pdf}
}

@article{yao_survey_2020,
	title = {A {Survey} on {Causal} {Inference}},
	url = {http://arxiv.org/abs/2002.02770},
	abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well known causal inference framework. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
	urldate = {2020-03-06},
	journal = {arXiv:2002.02770 [cs, stat]},
	author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.02770},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	file = {arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\9RBBDZEK\\2002.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\MDVV9VNJ\\Yao et al. - 2020 - A Survey on Causal Inference.pdf:application/pdf}
}

@article{mohseni_multidisciplinary_2019,
	title = {A {Multidisciplinary} {Survey} and {Framework} for {Design} and {Evaluation} of {Explainable} {AI} {Systems}},
	url = {http://arxiv.org/abs/1811.11839},
	abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems users. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge from across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation method in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation loop in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
	urldate = {2020-03-06},
	journal = {arXiv:1811.11839 [cs]},
	author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
	month = dec,
	year = {2019},
	note = {arXiv: 1811.11839},
	keywords = {Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\FMFAU3UL\\1811.html:text/html;arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\LKBRR82H\\Mohseni et al. - 2019 - A Multidisciplinary Survey and Framework for Desig.pdf:application/pdf}
}

@article{arik_tabnet_2019,
	title = {{TabNet}: {Attentive} {Interpretable} {Tabular} {Learning}},
	journal = {arXiv preprint arXiv:1908.07442},
	author = {Arik, Sercan O and Pfister, Tomas},
	year = {2019},
	file = {Arik and Pfister - 2019 - TabNet Attentive Interpretable Tabular Learning.pdf:C\:\\Users\\13762012\\Zotero\\storage\\X4A7K2TY\\Arik and Pfister - 2019 - TabNet Attentive Interpretable Tabular Learning.pdf:application/pdf}
}

@article{zhang_explainable_2018,
	title = {Explainable recommendation: {A} survey and new perspectives},
	journal = {arXiv preprint arXiv:1804.11192},
	author = {Zhang, Yongfeng and Chen, Xu},
	year = {2018}
}

@inproceedings{ribeiro__2016,
	title = {" {Why} should i trust you?" {Explaining} the predictions of any classifier},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} international conference on knowledge discovery and data mining},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year = {2016},
	pages = {1135--1144},
	file = {Ribeiro et al. - 2016 -  Why should i trust you Explaining the predicti.pdf:C\:\\Users\\13762012\\Zotero\\storage\\M8UT8Y5P\\Ribeiro et al. - 2016 -  Why should i trust you Explaining the predicti.pdf:application/pdf}
}

@inproceedings{ribeiro2018anchors,
  title={Anchors: High-precision model-agnostic explanations},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{guidotti2018local,
  title={Local rule-based explanations of black box decision systems},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  journal={arXiv preprint arXiv:1805.10820},
  year={2018}
}



@inproceedings{zang_learning_2018,
	address = {London United Kingdom},
	title = {Learning and {Interpreting} {Complex} {Distributions} in {Empirical} {Data}},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220073},
	doi = {10.1145/3219819.3220073},
	language = {en},
	urldate = {2020-03-09},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Zang, Chengxi and Cui, Peng and Zhu, Wenwu},
	month = jul,
	year = {2018},
	pages = {2682--2691},
	file = {Zang et al. - 2018 - Learning and Interpreting Complex Distributions in.pdf:C\:\\Users\\13762012\\Zotero\\storage\\GQAYTVWQ\\Zang et al. - 2018 - Learning and Interpreting Complex Distributions in.pdf:application/pdf}
}

@inproceedings{liu_interpretation_2018,
	address = {London United Kingdom},
	title = {On {Interpretation} of {Network} {Embedding} via {Taxonomy} {Induction}},
	isbn = {978-1-4503-5552-0},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220001},
	doi = {10.1145/3219819.3220001},
	language = {en},
	urldate = {2020-03-09},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Liu, Ninghao and Huang, Xiao and Li, Jundong and Hu, Xia},
	month = jul,
	year = {2018},
	pages = {1812--1820},
	file = {Liu et al. - 2018 - On Interpretation of Network Embedding via Taxonom.pdf:C\:\\Users\\13762012\\Zotero\\storage\\WZ8MBX48\\Liu et al. - 2018 - On Interpretation of Network Embedding via Taxonom.pdf:application/pdf}
}

@inproceedings{peake_explanation_2018,
	address = {London United Kingdom},
	title = {Explanation {Mining}: {Post} {Hoc} {Interpretability} of {Latent} {Factor} {Models} for {Recommendation} {Systems}},
	isbn = {978-1-4503-5552-0},
	shorttitle = {Explanation {Mining}},
	url = {https://dl.acm.org/doi/10.1145/3219819.3220072},
	doi = {10.1145/3219819.3220072},
	language = {en},
	urldate = {2020-03-09},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Peake, Georgina and Wang, Jun},
	month = jul,
	year = {2018},
	pages = {2060--2069},
	file = {Peake and Wang - 2018 - Explanation Mining Post Hoc Interpretability of L.pdf:C\:\\Users\\13762012\\Zotero\\storage\\RVNQ4YVN\\Peake and Wang - 2018 - Explanation Mining Post Hoc Interpretability of L.pdf:application/pdf}
}

@inproceedings{tolomei_interpretable_2017,
	address = {Halifax, NS, Canada},
	title = {Interpretable {Predictions} of {Tree}-based {Ensembles} via {Actionable} {Feature} {Tweaking}},
	isbn = {978-1-4503-4887-4},
	url = {http://dl.acm.org/citation.cfm?doid=3097983.3098039},
	doi = {10.1145/3097983.3098039},
	language = {en},
	urldate = {2020-03-09},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}  - {KDD} '17},
	publisher = {ACM Press},
	author = {Tolomei, Gabriele and Silvestri, Fabrizio and Haines, Andrew and Lalmas, Mounia},
	year = {2017},
	pages = {465--474},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\K9M5FAF2\\Tolomei et al. - 2017 - Interpretable Predictions of Tree-based Ensembles .pdf:application/pdf}
}

@article{ai_learning_2018,
	title = {Learning {Heterogeneous} {Knowledge} {Base} {Embeddings} for {Explainable} {Recommendation}},
	volume = {11},
	issn = {1999-4893},
	url = {http://www.mdpi.com/1999-4893/11/9/137},
	doi = {10.3390/a11090137},
	abstract = {Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms—especially the collaborative filtering (CF)- based approaches with shallow or deep models—usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users’ historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines.},
	language = {en},
	number = {9},
	urldate = {2020-03-09},
	journal = {Algorithms},
	author = {Ai, Qingyao and Azizi, Vahid and Chen, Xu and Zhang, Yongfeng},
	month = sep,
	year = {2018},
	pages = {137},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\P96N7K3B\\Ai et al. - 2018 - Learning Heterogeneous Knowledge Base Embeddings f.pdf:application/pdf}
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {00043702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	language = {en},
	urldate = {2020-03-10},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	pages = {1--38},
	file = {Submitted Version:C\:\\Users\\13762012\\Zotero\\storage\\JET7U2EL\\Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf;1706.07269.pdf:C\:\\Users\\13762012\\Zotero\\storage\\YTI8RT4A\\1706.07269.pdf:application/pdf}
}

@article{miller_but_2019,
	title = {"{But} why?": understanding explainable artificial intelligence},
	volume = {25},
	issn = {15284972},
	shorttitle = {"{But} why?},
	url = {http://dl.acm.org/citation.cfm?doid=3325198.3313107},
	doi = {10.1145/3313107},
	language = {en},
	number = {3},
	urldate = {2020-03-10},
	journal = {XRDS: Crossroads, The ACM Magazine for Students},
	author = {Miller, Tim},
	month = apr,
	year = {2019},
	pages = {20--25}
}

@inproceedings{alvarez-melis_causal_2017,
	address = {Copenhagen, Denmark},
	title = {A causal framework for explaining the predictions of black-box sequence-to-sequence models},
	url = {http://aclweb.org/anthology/D17-1042},
	doi = {10.18653/v1/D17-1042},
	language = {en},
	urldate = {2020-03-10},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Alvarez-Melis, David and Jaakkola, Tommi},
	year = {2017},
	pages = {412--421},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\CSYPWEF9\\Alvarez-Melis and Jaakkola - 2017 - A causal framework for explaining the predictions .pdf:application/pdf;D17-1042.pdf:C\:\\Users\\13762012\\Zotero\\storage\\8MEMXMI2\\D17-1042.pdf:application/pdf}
}

@article{ghorbani_towards_2019,
	title = {Towards {Automatic} {Concept}-based {Explanations}},
	url = {http://arxiv.org/abs/1902.03129},
	abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for {\textbackslash}emph\{concept\} based explanation, which goes beyond per-sample features to identify higher-level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that {\textbackslash}alg discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
	urldate = {2020-03-10},
	journal = {arXiv:1902.03129 [cs, stat]},
	author = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
	month = oct,
	year = {2019},
	note = {arXiv: 1902.03129},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\LDG93WZH\\Ghorbani et al. - 2019 - Towards Automatic Concept-based Explanations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\8AEQGAGT\\1902.html:text/html}
}

@article{saumitra_mishra_local_2017,
	title = {Local {Interpretable} {Model}-{Agnostic} {Explanations} {For} {Music} {Content} {Analysis}.},
	copyright = {Creative Commons Attribution 4.0, Open Access},
	url = {https://zenodo.org/record/1417387},
	doi = {10.5281/ZENODO.1417387},
	abstract = {[TODO] Add abstract here.},
	urldate = {2020-03-27},
	author = {Saumitra Mishra and Sturm, Bob L. and Dixon, Simon},
	month = oct,
	year = {2017},
	note = {Publisher: Zenodo},
	file = {MishraSD17.pdf:C\:\\Users\\13762012\\Downloads\\MishraSD17.pdf:application/pdf}
}

@article{lucic_explaining_2019,
	title = {Explaining {Predictions} from {Tree}-based {Boosting} {Ensembles}},
	url = {http://arxiv.org/abs/1907.02582},
	abstract = {Understanding how "black-box" models arrive at their predictions has sparked significant interest from both within and outside the AI community. Our work focuses on doing this by generating local explanations about individual predictions for tree-based ensembles, specifically Gradient Boosting Decision Trees (GBDTs). Given a correctly predicted instance in the training set, we wish to generate a counterfactual explanation for this instance, that is, the minimal perturbation of this instance such that the prediction flips to the opposite class. Most existing methods for counterfactual explanations are (1) model-agnostic, so they do not take into account the structure of the original model, and/or (2) involve building a surrogate model on top of the original model, which is not guaranteed to represent the original model accurately. There exists a method specifically for random forests; we wish to extend this method for GBDTs. This involves accounting for (1) the sequential dependency between trees and (2) training on the negative gradients instead of the original labels.},
	urldate = {2020-03-16},
	journal = {arXiv:1907.02582 [cs, stat]},
	author = {Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.02582},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	annote = {Comment: SIGIR 2019: FACTS-IR Workshop},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\Z92QAL4G\\Lucic et al. - 2019 - Explaining Predictions from Tree-based Boosting En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\F2RJZA73\\1907.html:text/html}
}


@article{jiang_trust_2018,
	title = {To {Trust} {Or} {Not} {To} {Trust} {A} {Classifier}},
	url = {http://arxiv.org/abs/1805.11783},
	abstract = {Knowing when a classifier's prediction can be trusted is useful in many applications and critical for safely using AI. While the bulk of the effort in machine learning research has been towards improving classifier performance, understanding when a classifier's predictions should and should not be trusted has received far less attention. The standard approach is to use the classifier's discriminant or confidence score; however, we show there exists an alternative that is more effective in many situations. We propose a new score, called the trust score, which measures the agreement between the classifier and a modified nearest-neighbor classifier on the testing example. We show empirically that high (low) trust scores produce surprisingly high precision at identifying correctly (incorrectly) classified examples, consistently outperforming the classifier's confidence score as well as many other baselines. Further, under some mild distributional assumptions, we show that if the trust score for an example is high (low), the classifier will likely agree (disagree) with the Bayes-optimal classifier. Our guarantees consist of non-asymptotic rates of statistical consistency under various nonparametric settings and build on recent developments in topological data analysis.},
	urldate = {2020-03-10},
	journal = {arXiv:1805.11783 [cs, stat]},
	author = {Jiang, Heinrich and Kim, Been and Guan, Melody Y. and Gupta, Maya},
	month = oct,
	year = {2018},
	note = {arXiv: 1805.11783},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\E6CXEW3E\\Jiang et al. - 2018 - To Trust Or Not To Trust A Classifier.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\AJMH3UXB\\1805.html:text/html}
}

@article{holzinger_causability_2019,
	title = {Causability and explainability of artificial intelligence in medicine},
	volume = {9},
	issn = {1942-4787, 1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312},
	doi = {10.1002/widm.1312},
	language = {en},
	number = {4},
	urldate = {2020-03-10},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
	month = jul,
	year = {2019},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\NWU2YIVI\\Holzinger et al. - 2019 - Causability and explainability of artificial intel.pdf:application/pdf}
}

@article{kim_bayesian_2015,
	title = {The {Bayesian} {Case} {Model}: {A} {Generative} {Approach} for {Case}-{Based} {Reasoning} and {Prototype} {Classification}},
	shorttitle = {The {Bayesian} {Case} {Model}},
	url = {http://arxiv.org/abs/1503.01161},
	abstract = {We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the "quintessential" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.},
	urldate = {2020-03-10},
	journal = {arXiv:1503.01161 [cs, stat]},
	author = {Kim, Been and Rudin, Cynthia and Shah, Julie},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.01161},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in Neural Information Processing Systems (NIPS) 2014, Neural Information Processing Systems (NIPS) 2014},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\VBYUJIHT\\Kim et al. - 2015 - The Bayesian Case Model A Generative Approach for.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\L5NQY2PD\\1503.html:text/html}
}

@inproceedings{zhang_axiomatic_2019,
	address = {Anchorage, AK, USA},
	title = {Axiomatic {Interpretability} for {Multiclass} {Additive} {Models}},
	isbn = {978-1-4503-6201-6},
	url = {http://dl.acm.org/citation.cfm?doid=3292500.3330898},
	doi = {10.1145/3292500.3330898},
	language = {en},
	urldate = {2020-03-11},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}  - {KDD} '19},
	publisher = {ACM Press},
	author = {Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},
	year = {2019},
	pages = {226--234},
	file = {Zhang et al. - 2019 - Axiomatic Interpretability for Multiclass Additive.pdf:C\:\\Users\\13762012\\Zotero\\storage\\29MKLFWX\\Zhang et al. - 2019 - Axiomatic Interpretability for Multiclass Additive.pdf:application/pdf}
}

@article{wachter_counterfactual_2018,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {http://arxiv.org/abs/1711.00399},
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	urldate = {2020-03-11},
	journal = {arXiv:1711.00399 [cs]},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.00399},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\6PJIL6G8\\Wachter et al. - 2018 - Counterfactual Explanations without Opening the Bl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\5LWQWLIY\\1711.html:text/html}
}

@article{van_looveren_interpretable_2020,
	title = {Interpretable {Counterfactual} {Explanations} {Guided} by {Prototypes}},
	url = {http://arxiv.org/abs/1907.02584},
	abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the the search for counterfactual instances and result in more interpretable explanations. We introduce two novel metrics to quantitatively evaluate local interpretability at the instance level. We use these metrics to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). The method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for \${\textbackslash}textit\{black box\}\$ models.},
	urldate = {2020-03-11},
	journal = {arXiv:1907.02584 [cs, stat]},
	author = {Van Looveren, Arnaud and Klaise, Janis},
	month = feb,
	year = {2020},
	note = {arXiv: 1907.02584},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 13 figures. For an open source implementation of the algorithm, see https://github.com/SeldonIO/alibi},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\P3VKVSGB\\Van Looveren and Klaise - 2020 - Interpretable Counterfactual Explanations Guided b.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\CRC2URFF\\1907.html:text/html;1907.02584.pdf:C\:\\Users\\13762012\\Zotero\\storage\\IG648XZS\\1907.02584.pdf:application/pdf}
}

@inproceedings{mothilal_explaining_2020,
	address = {Barcelona Spain},
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372850},
	doi = {10.1145/3351095.3372850},
	language = {en},
	urldate = {2020-03-11},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	pages = {607--617},
	file = {Submitted Version:C\:\\Users\\13762012\\Zotero\\storage\\2IK395J4\\Mothilal et al. - 2020 - Explaining machine learning classifiers through di.pdf:application/pdf}
}

@article{hayes_causal_2018,
	title = {Causal explanation improves judgment under uncertainty, but rarely in a {Bayesian} way},
	volume = {46},
	issn = {0090-502X, 1532-5946},
	url = {http://link.springer.com/10.3758/s13421-017-0750-z},
	doi = {10.3758/s13421-017-0750-z},
	language = {en},
	number = {1},
	urldate = {2020-03-11},
	journal = {Memory \& Cognition},
	author = {Hayes, Brett K. and Ngo, Jeremy and Hawkins, Guy E. and Newell, Ben R.},
	month = jan,
	year = {2018},
	pages = {112--131},
	file = {Hayes et al. - 2018 - Causal explanation improves judgment under uncerta.pdf:C\:\\Users\\13762012\\Zotero\\storage\\AKM5JQDL\\Hayes et al. - 2018 - Causal explanation improves judgment under uncerta.pdf:application/pdf}
}

@article{schwab_cxplain_2019,
	title = {{CXPlain}: {Causal} {Explanations} for {Model} {Interpretation} under {Uncertainty}},
	shorttitle = {{CXPlain}},
	url = {http://arxiv.org/abs/1910.12336},
	abstract = {Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
	urldate = {2020-03-11},
	journal = {arXiv:1910.12336 [cs, stat]},
	author = {Schwab, Patrick and Karlen, Walter},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.12336},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in Advances in Neural Information Processing Systems 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\5JHS2E7S\\Schwab and Karlen - 2019 - CXPlain Causal Explanations for Model Interpretat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\KMYCRC3X\\1910.html:text/html}
}

@article{wang_multi-value_2017,
	title = {Multi-{Value} {Rule} {Sets}},
	url = {http://arxiv.org/abs/1710.05257},
	abstract = {We present the Multi-vAlue Rule Set (MARS) model for interpretable classification with feature efficient presentations. MARS introduces a more generalized form of association rules that allows multiple values in a condition. Rules of this form are more concise than traditional single-valued rules in capturing and describing patterns in data. MARS mitigates the problem of dealing with continuous features and high-cardinality categorical features faced by rule-based models. Our formulation also pursues a higher efficiency of feature utilization, which reduces the cognitive load to understand the decision process. We propose an efficient inference method for learning a maximum a posteriori model, incorporating theoretically grounded bounds to iteratively reduce the search space to improve search efficiency. Experiments with synthetic and real-world data demonstrate that MARS models have significantly smaller complexity and fewer features, providing better interpretability while being competitive in predictive accuracy. We conducted a usability study with human subjects and results show that MARS is the easiest to use compared with other competing rule-based models, in terms of the correct rate and response time. Overall, MARS introduces a new approach to rule-based models that balance accuracy and interpretability with feature-efficient representations.},
	urldate = {2020-03-11},
	journal = {arXiv:1710.05257 [cs], NIPS},
	author = {Wang, Tong},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05257},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\S8TCDR7Z\\Wang - 2017 - Multi-Value Rule Sets.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\GYVHS9II\\1710.html:text/html}
}

@article{kim_learning_2019,
	title = {Learning {Interpretable} {Models} with {Causal} {Guarantees}},
	url = {http://arxiv.org/abs/1901.08576},
	abstract = {Machine learning has shown much promise in helping improve the quality of medical, legal, and economic decision-making. In these applications, machine learning models must satisfy two important criteria: (i) they must be causal, since the goal is typically to predict individual treatment effects, and (ii) they must be interpretable, so that human decision makers can validate and trust the model predictions. There has recently been much progress along each direction independently, yet the state-of-the-art approaches are fundamentally incompatible. We propose a framework for learning causal interpretable models---from observational data---that can be used to predict individual treatment effects. Our framework can be used with any algorithm for learning interpretable models. Furthermore, we prove an error bound on the treatment effects predicted by our model. Finally, in an experiment on real-world data, we show that the models trained using our framework significantly outperform a number of baselines.},
	urldate = {2020-03-12},
	journal = {arXiv:1901.08576 [cs, stat]},
	author = {Kim, Carolyn and Bastani, Osbert},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.08576},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\ZHX4KVJB\\Kim and Bastani - 2019 - Learning Interpretable Models with Causal Guarante.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\CZZXHCVB\\1901.html:text/html}
}

@article{goyal_counterfactual_2019,
	title = {Counterfactual {Visual} {Explanations}},
	url = {http://arxiv.org/abs/1904.07451},
	abstract = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image \$I\$ for which a vision system predicts class \$c\$, a counterfactual visual explanation identifies how \$I\$ could change such that the system would output a different specified class \$c'\$. To do this, we select a 'distractor' image \$I'\$ that the system predicts as class \$c'\$ and identify spatial regions in \$I\$ and \$I'\$ such that replacing the identified region in \$I\$ with the identified region in \$I'\$ would push the system towards classifying \$I\$ as \$c'\$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.},
	urldate = {2020-03-12},
	journal = {arXiv:1904.07451 [cs, stat], ICML},
	author = {Goyal, Yash and Wu, Ziyan and Ernst, Jan and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.07451},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\TPB9XAQK\\Goyal et al. - 2019 - Counterfactual Visual Explanations.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\6Q7JZ2QG\\1904.html:text/html}
}

@article{ancona_explaining_2019,
	title = {Explaining {Deep} {Neural} {Networks} with a {Polynomial} {Time} {Algorithm} for {Shapley} {Values} {Approximation}},
	url = {http://arxiv.org/abs/1903.10992},
	abstract = {The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.},
	urldate = {2020-03-12},
	journal = {arXiv:1903.10992 [cs, stat]},
	author = {Ancona, Marco and Öztireli, Cengiz and Gross, Markus},
	month = jun,
	year = {2019},
	note = {arXiv: 1903.10992},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\IVWB5B8G\\Ancona et al. - 2019 - Explaining Deep Neural Networks with a Polynomial .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\8BIETJNT\\1903.html:text/html}
}

@article{chen_learning_2018,
	title = {Learning to {Explain}: {An} {Information}-{Theoretic} {Perspective} on {Model} {Interpretation}},
	shorttitle = {Learning to {Explain}},
	url = {http://arxiv.org/abs/1802.07814},
	abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
	urldate = {2020-03-12},
	journal = {arXiv:1802.07814 [cs, stat], ICML 2018},
	author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.07814},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ICML 2018 as a long oral},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\57F9DJHW\\Chen et al. - 2018 - Learning to Explain An Information-Theoretic Pers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\NWSB4LGZ\\1802.html:text/html}
}

@inproceedings{hara_making_2018,
	address = {Playa Blanca, Lanzarote, Canary Islands},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Making {Tree} {Ensembles} {Interpretable}: {A} {Bayesian} {Model} {Selection} {Approach}},
	volume = {84},
	url = {http://proceedings.mlr.press/v84/hara18a.html},
	abstract = {Tree ensembles, such as random forests, are renowned for their high prediction performance. However, their interpretability is critically limited due to the enormous complexity. In this study, we propose a method to make a complex tree ensemble interpretable by simplifying the model. Specifically, we formalize the simplification of tree ensembles as a model selection problem. Given a complex tree ensemble, we aim at obtaining the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm that optimizes the simplified model while maintaining the prediction performance. Our numerical experiments on several datasets showed that complicated tree ensembles were approximated interpretably.},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Hara, Satoshi and Hayashi, Kohei},
	editor = {Storkey, Amos and Perez-Cruz, Fernando},
	month = apr,
	year = {2018},
	pages = {77--85}
}

@article{fox_explainable_2017,
	title = {Explainable {Planning}},
	url = {http://arxiv.org/abs/1709.10256},
	abstract = {As AI is increasingly being adopted into application solutions, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support integrated working styles, in which humans and intelligent systems cooperate in problem-solving, but also it is a necessary step in the process of building trust as humans migrate greater responsibility to such systems. The challenge is to find effective ways to communicate the foundations of AI-driven behaviour, when the algorithms that drive it are far from transparent to humans. In this paper we consider the opportunities that arise in AI planning, exploiting the model-based representations that form a familiar and common basis for communication with users, while acknowledging the gap between planning algorithms and human problem-solving.},
	urldate = {2020-03-13},
	journal = {arXiv:1709.10256 [cs]},
	author = {Fox, Maria and Long, Derek and Magazzeni, Daniele},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.10256},
	keywords = {Computer Science - Artificial Intelligence, I.2, I.2.9},
	annote = {Comment: Presented at the IJCAI-17 workshop on Explainable AI (http://home.earthlink.net/{\textasciitilde}dwaha/research/meetings/ijcai17-xai/). Melbourne, August 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\2LZIAHUK\\Fox et al. - 2017 - Explainable Planning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\HEXDQT2G\\1709.html:text/html}
}

@article{gilpin_explaining_2019,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/1806.00069},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	urldate = {2020-03-13},
	journal = {arXiv:1806.00069 [cs, stat]},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.00069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: The 5th IEEE International Conference on Data Science and Advanced Analytics (DSAA 2018). [Research Track]},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\LTPMJMLT\\Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\HZYT4DRT\\1806.html:text/html}
}

@article{kailkhura_reliable_2019,
	title = {Reliable and explainable machine-learning methods for accelerated material discovery},
	volume = {5},
	issn = {2057-3960},
	url = {http://www.nature.com/articles/s41524-019-0248-2},
	doi = {10.1038/s41524-019-0248-2},
	language = {en},
	number = {1},
	urldate = {2020-03-13},
	journal = {npj Computational Materials},
	author = {Kailkhura, Bhavya and Gallagher, Brian and Kim, Sookyung and Hiszpanski, Anna and Han, T. Yong-Jin},
	month = dec,
	year = {2019},
	pages = {108},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\H2CVRG4K\\Kailkhura et al. - 2019 - Reliable and explainable machine-learning methods .pdf:application/pdf}
}

@article{timonen_interpretable_2019,
	title = {An interpretable probabilistic machine learning method for heterogeneous longitudinal studies},
	url = {http://arxiv.org/abs/1912.03549},
	abstract = {Identifying risk factors from longitudinal data requires statistical tools that are not restricted to linear models, yet provide interpretable associations between different types of covariates and a response variable. Here, we present a widely applicable and interpretable probabilistic machine learning method for nonparametric longitudinal data analysis using additive Gaussian process regression. We demonstrate that it outperforms previous longitudinal modeling approaches and provides useful novel features, including the ability to account for uncertainty in disease effect times as well as heterogeneity in their effects.},
	urldate = {2020-03-13},
	journal = {arXiv:1912.03549 [cs, q-bio, stat]},
	author = {Timonen, Juho and Mannerström, Henrik and Vehtari, Aki and Lähdesmäki, Harri},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.03549},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, Quantitative Biology - Quantitative Methods},
	annote = {Comment: 25 pages, 10 Figures. Tables S1-S2 in an ancillary file},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\QC8A3YDN\\Timonen et al. - 2019 - An interpretable probabilistic machine learning me.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\K5R958JU\\1912.html:text/html}
}

@inproceedings{li_learning_2019,
	address = {Macao, China},
	title = {Learning {Interpretable} {Deep} {State} {Space} {Model} for {Probabilistic} {Time} {Series} {Forecasting}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/402},
	doi = {10.24963/ijcai.2019/402},
	abstract = {Probabilistic time series forecasting involves estimating the distribution of future based on its history, which is essential for risk management in downstream decision-making. We propose a deep state space model for probabilistic time series forecasting whereby the non-linear emission model and transition model are parameterized by networks and the dependency is modeled by recurrent neural nets. We take the automatic relevance determination (ARD) view and devise a network to exploit the exogenous variables in addition to time series. In particular, our ARD network can incorporate the uncertainty of the exogenous variables and eventually helps identify useful exogenous variables and suppress those irrelevant for forecasting. The distribution of multi-step ahead forecasts are approximated by Monte Carlo simulation. We show in experiments that our model produces accurate and sharp probabilistic forecasts. The estimated uncertainty of our forecasting also realistically increases over time, in a spontaneous manner.},
	language = {en},
	urldate = {2020-03-13},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Li, Longyuan and Yan, Junchi and Yang, Xiaokang and Jin, Yaohui},
	month = aug,
	year = {2019},
	pages = {2901--2908}
}

@incollection{filchenkov_interpretable_2018,
	address = {Cham},
	title = {Interpretable {Probabilistic} {Embeddings}: {Bridging} the {Gap} {Between} {Topic} {Models} and {Neural} {Networks}},
	volume = {789},
	isbn = {978-3-319-71745-6 978-3-319-71746-3},
	shorttitle = {Interpretable {Probabilistic} {Embeddings}},
	url = {http://link.springer.com/10.1007/978-3-319-71746-3_15},
	urldate = {2020-03-13},
	booktitle = {Artificial {Intelligence} and {Natural} {Language}},
	publisher = {Springer International Publishing},
	author = {Potapenko, Anna and Popov, Artem and Vorontsov, Konstantin},
	editor = {Filchenkov, Andrey and Pivovarova, Lidia and Žižka, Jan},
	year = {2018},
	doi = {10.1007/978-3-319-71746-3_15},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {167--180},
	file = {Submitted Version:C\:\\Users\\13762012\\Zotero\\storage\\ZSXJJ9Y6\\Potapenko et al. - 2018 - Interpretable Probabilistic Embeddings Bridging t.pdf:application/pdf}
}

@article{odom_human-guided_2018,
	title = {Human-{Guided} {Learning} for {Probabilistic} {Logic} {Models}},
	volume = {5},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/article/10.3389/frobt.2018.00056/full},
	doi = {10.3389/frobt.2018.00056},
	urldate = {2020-03-13},
	journal = {Frontiers in Robotics and AI},
	author = {Odom, Phillip and Natarajan, Sriraam},
	month = jun,
	year = {2018},
	pages = {56},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\WH9ITLQ7\\Odom and Natarajan - 2018 - Human-Guided Learning for Probabilistic Logic Mode.pdf:application/pdf}
}

@article{levray_learning_2019,
	title = {Learning {Tractable} {Probabilistic} {Models} in {Open} {Worlds}},
	url = {http://arxiv.org/abs/1901.05847},
	abstract = {Large-scale probabilistic representations, including statistical knowledge bases and graphical models, are increasingly in demand. They are built by mining massive sources of structured and unstructured data, the latter often derived from natural language processing techniques. The very nature of the enterprise makes the extracted representations probabilistic. In particular, inducing relations and facts from noisy and incomplete sources via statistical machine learning models means that the labels are either already probabilistic, or that probabilities approximate confidence. While the progress is impressive, extracted representations essentially enforce the closed-world assumption, which means that all facts in the database are accorded the corresponding probability, but all other facts have probability zero. The CWA is deeply problematic in most machine learning contexts. A principled solution is needed for representing incomplete and indeterminate knowledge in such models, imprecise probability models such as credal networks being an example. In this work, we are interested in the foundational problem of learning such open-world probabilistic models. However, since exact inference in probabilistic graphical models is intractable, the paradigm of tractable learning has emerged to learn data structures (such as arithmetic circuits) that support efficient probabilistic querying. We show here how the computational machinery underlying tractable learning has to be generalized for imprecise probabilities. Our empirical evaluations demonstrate that our regime is also effective.},
	urldate = {2020-03-13},
	journal = {arXiv:1901.05847 [cs, stat]},
	author = {Levray, Amelie and Belle, Vaishak},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.05847},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\PTG9R6GT\\Levray and Belle - 2019 - Learning Tractable Probabilistic Models in Open Wo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\6TWHL9FF\\1901.html:text/html}
}

@article{letham_interpretable_2015,
	title = {Interpretable classifiers using rules and {Bayesian} analysis: {Building} a better stroke prediction model},
	volume = {9},
	issn = {1932-6157},
	shorttitle = {Interpretable classifiers using rules and {Bayesian} analysis},
	url = {http://arxiv.org/abs/1511.01644},
	doi = {10.1214/15-AOAS848},
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if...then... statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS\$\_2\$ score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS\$\_2\$, but more accurate.},
	number = {3},
	urldate = {2020-03-13},
	journal = {The Annals of Applied Statistics},
	author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
	month = sep,
	year = {2015},
	note = {arXiv: 1511.01644},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Applications},
	pages = {1350--1371},
	annote = {Comment: Published at http://dx.doi.org/10.1214/15-AOAS848 in the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\RD6N2B2X\\Letham et al. - 2015 - Interpretable classifiers using rules and Bayesian.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\PJ936NQ7\\1511.html:text/html}
}

@article{arrieta_explainable_2019,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, {Taxonomies}, {Opportunities} and {Challenges} toward {Responsible} {AI}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {http://arxiv.org/abs/1910.10045},
	abstract = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	urldate = {2020-03-13},
	journal = {arXiv:1910.10045 [cs]},
	author = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = dec,
	year = {2019},
	note = {arXiv: 1910.10045},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 67 pages, 13 figures, accepted for its publication in Information Fusion},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\67QKEGDR\\Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\VFTIPTHB\\1910.html:text/html}
}

@article{harder_interpretable_2019,
	title = {Interpretable and {Differentially} {Private} {Predictions}},
	url = {http://arxiv.org/abs/1906.02004},
	abstract = {Interpretable predictions, where it is clear why a machine learning model has made a particular decision, can compromise privacy by revealing the characteristics of individual data points. This raises the central question addressed in this paper: Can models be interpretable without compromising privacy? For complex big data fit by correspondingly rich models, balancing privacy and explainability is particularly challenging, such that this question has remained largely unexplored. In this paper, we propose a family of simple models in the aim of approximating complex models using several locally linear maps per class to provide high classification accuracy, as well as differentially private explanations on the classification. We illustrate the usefulness of our approach on several image benchmark datasets as well as a medical dataset.},
	urldate = {2020-03-14},
	journal = {arXiv:1906.02004 [cs, stat]},
	author = {Harder, Frederik and Bauer, Matthias and Park, Mijung},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.02004},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\H2BHSFS9\\Harder et al. - 2019 - Interpretable and Differentially Private Predictio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\WKJSN7X2\\1906.html:text/html}
}

@article{dong_asymmetrical_2019,
	title = {Asymmetrical {Hierarchical} {Networks} with {Attentive} {Interactions} for {Interpretable} {Review}-{Based} {Recommendation}},
	url = {http://arxiv.org/abs/2001.04346},
	abstract = {Recently, recommender systems have been able to emit substantially improved recommendations by leveraging user-provided reviews. Existing methods typically merge all reviews of a given user or item into a long document, and then process user and item documents in the same manner. In practice, however, these two sets of reviews are notably different: users' reviews reflect a variety of items that they have bought and are hence very heterogeneous in their topics, while an item's reviews pertain only to that single item and are thus topically homogeneous. In this work, we develop a novel neural network model that properly accounts for this important difference by means of asymmetric attentive modules. The user module learns to attend to only those signals that are relevant with respect to the target item, whereas the item module learns to extract the most salient contents with regard to properties of the item. Our multi-hierarchical paradigm accounts for the fact that neither are all reviews equally useful, nor are all sentences within each review equally pertinent. Extensive experimental results on a variety of real datasets demonstrate the effectiveness of our method.},
	urldate = {2020-03-14},
	journal = {arXiv:2001.04346 [cs], AAAI20},
	author = {Dong, Xin and Ni, Jingchao and Cheng, Wei and Chen, Zhengzhang and Zong, Bo and Song, Dongjin and Liu, Yanchi and Chen, Haifeng and de Melo, Gerard},
	month = dec,
	year = {2019},
	note = {arXiv: 2001.04346},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\9J826XVR\\Dong et al. - 2019 - Asymmetrical Hierarchical Networks with Attentive .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\6A97XSHA\\2001.html:text/html}
}

@article{lundberg_explainable_2019,
	title = {Explainable {AI} for {Trees}: {From} {Local} {Explanations} to {Global} {Understanding}},
	shorttitle = {Explainable {AI} for {Trees}},
	url = {http://arxiv.org/abs/1905.04610},
	abstract = {Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.},
	urldate = {2020-03-15},
	journal = {arXiv:1905.04610 [cs, stat]},
	author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	month = may,
	year = {2019},
	note = {arXiv: 1905.04610},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\Z47C5JZ8\\Lundberg et al. - 2019 - Explainable AI for Trees From Local Explanations .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\F3QRWI54\\1905.html:text/html}
}

@article{du_techniques_2019,
	title = {Techniques for {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1808.00033},
	abstract = {Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.},
	urldate = {2020-03-15},
	journal = {arXiv:1808.00033 [cs, stat]},
	author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
	month = may,
	year = {2019},
	note = {arXiv: 1808.00033},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, read, good},
	annote = {Comment: Accepted by Communications of the ACM (CACM), Review Article},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\U4K6CJ84\\Du et al. - 2019 - Techniques for Interpretable Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\PAP73CE7\\1808.html:text/html}
}

@article{klein_explaining_2018,
	title = {Explaining {Explanation}, {Part} 3: {The} {Causal} {Landscape}},
	volume = {33},
	issn = {1541-1672},
	shorttitle = {Explaining {Explanation}, {Part} 3},
	url = {https://ieeexplore.ieee.org/document/8378482/},
	doi = {10.1109/MIS.2018.022441353},
	number = {2},
	urldate = {2020-03-15},
	journal = {IEEE Intelligent Systems},
	author = {Klein, Gary},
	month = mar,
	year = {2018},
	pages = {83--88},
	file = {Klein - 2018 - Explaining Explanation, Part 3 The Causal Landsca.pdf:C\:\\Users\\13762012\\Zotero\\storage\\BVAIE8CH\\Klein - 2018 - Explaining Explanation, Part 3 The Causal Landsca.pdf:application/pdf}
}

@article{lucic_explaining_2019,
	title = {Explaining {Predictions} from {Tree}-based {Boosting} {Ensembles}},
	url = {http://arxiv.org/abs/1907.02582},
	abstract = {Understanding how "black-box" models arrive at their predictions has sparked significant interest from both within and outside the AI community. Our work focuses on doing this by generating local explanations about individual predictions for tree-based ensembles, specifically Gradient Boosting Decision Trees (GBDTs). Given a correctly predicted instance in the training set, we wish to generate a counterfactual explanation for this instance, that is, the minimal perturbation of this instance such that the prediction flips to the opposite class. Most existing methods for counterfactual explanations are (1) model-agnostic, so they do not take into account the structure of the original model, and/or (2) involve building a surrogate model on top of the original model, which is not guaranteed to represent the original model accurately. There exists a method specifically for random forests; we wish to extend this method for GBDTs. This involves accounting for (1) the sequential dependency between trees and (2) training on the negative gradients instead of the original labels.},
	urldate = {2020-03-16},
	journal = {arXiv:1907.02582 [cs, stat]},
	author = {Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.02582},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	annote = {Comment: SIGIR 2019: FACTS-IR Workshop},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\Z92QAL4G\\Lucic et al. - 2019 - Explaining Predictions from Tree-based Boosting En.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\F2RJZA73\\1907.html:text/html}
}

@article{bastani_interpretability_2018,
	title = {Interpretability via {Model} {Extraction}},
	url = {http://arxiv.org/abs/1706.09773},
	abstract = {The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.},
	urldate = {2020-03-16},
	journal = {arXiv:1706.09773 [cs, stat]},
	author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
	month = mar,
	year = {2018},
	note = {arXiv: 1706.09773},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	annote = {Comment: Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2017)},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\BK238R74\\Bastani et al. - 2018 - Interpretability via Model Extraction.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\88NBYM66\\1706.html:text/html}
}

@article{ross_right_2017,
	title = {Right for the {Right} {Reasons}: {Training} {Differentiable} {Models} by {Constraining} their {Explanations}},
	shorttitle = {Right for the {Right} {Reasons}},
	url = {http://arxiv.org/abs/1703.03717},
	abstract = {Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.},
	urldate = {2020-03-17},
	journal = {arXiv:1703.03717 [cs, stat]},
	author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
	month = may,
	year = {2017},
	note = {arXiv: 1703.03717},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\G3SHPH96\\Ross et al. - 2017 - Right for the Right Reasons Training Differentiab.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\35MSTZGV\\1703.html:text/html}
}

@article{moraffah_causal_2020,
	title = {Causal {Interpretability} for {Machine} {Learning} -- {Problems}, {Methods} and {Evaluation}},
	url = {http://arxiv.org/abs/2003.03934},
	abstract = {Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more human-friendly explanations, recent work on interpretability tries to answer questions related to causality such as {\textbackslash}Why does this model makes such decisions?" or {\textbackslash}Was it a specific feature that caused the decision made by the model?". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.},
	urldate = {2020-03-17},
	journal = {arXiv:2003.03934 [cs, stat]},
	author = {Moraffah, Raha and Karami, Mansooreh and Guo, Ruocheng and Raglin, Adrienne and Liu, Huan},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03934},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\YHVV8KYI\\Moraffah et al. - 2020 - Causal Interpretability for Machine Learning -- Pr.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\LG2E4U8G\\2003.html:text/html}
}

@article{pan_interpretable_2020,
	title = {Interpretable {Companions} for {Black}-{Box} {Models}},
	url = {http://arxiv.org/abs/2002.03494},
	abstract = {We present an interpretable companion model for any pre-trained black-box classifiers. The idea is that for any input, a user can decide to either receive a prediction from the black-box model, with high accuracy but no explanations, or employ a companion rule to obtain an interpretable prediction with slightly lower accuracy. The companion model is trained from data and the predictions of the black-box model, with the objective combining area under the transparency--accuracy curve and model complexity. Our model provides flexible choices for practitioners who face the dilemma of choosing between always using interpretable models and always using black-box models for a predictive task, so users can, for any given input, take a step back to resort to an interpretable prediction if they find the predictive performance satisfying, or stick to the black-box model if the rules are unsatisfying. To show the value of companion models, we design a human evaluation on more than a hundred people to investigate the tolerable accuracy loss to gain interpretability for humans.},
	urldate = {2020-03-18},
	journal = {arXiv:2002.03494 [cs, stat]},
	author = {Pan, Danqing and Wang, Tong and Hara, Satoshi},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.03494},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 15 pages, 6 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\7DZDQ6RG\\Pan et al. - 2020 - Interpretable Companions for Black-Box Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\4L9N9WT7\\2002.html:text/html}
}

@article{grath_interpretable_2018,
	title = {Interpretable {Credit} {Application} {Predictions} {With} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1811.05245},
	abstract = {We predict credit applications with off-the-shelf, interchangeable black-box classifiers and we explain single predictions with counterfactual explanations. Counterfactual explanations expose the minimal changes required on the input data to obtain a different result e.g., approved vs rejected application. Despite their effectiveness, counterfactuals are mainly designed for changing an undesired outcome of a prediction i.e. loan rejected. Counterfactuals, however, can be difficult to interpret, especially when a high number of features are involved in the explanation. Our contribution is two-fold: i) we propose positive counterfactuals, i.e. we adapt counterfactual explanations to also explain accepted loan applications, and ii) we propose two weighting strategies to generate more interpretable counterfactuals. Experiments on the HELOC loan applications dataset show that our contribution outperforms the baseline counterfactual generation strategy, by leading to smaller and hence more interpretable counterfactuals.},
	urldate = {2020-03-22},
	journal = {arXiv:1811.05245 [cs]},
	author = {Grath, Rory Mc and Costabello, Luca and Van, Chan Le and Sweeney, Paul and Kamiab, Farbod and Shen, Zhao and Lecue, Freddy},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.05245},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\SZWU8VFU\\Grath et al. - 2018 - Interpretable Credit Application Predictions With .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\Y885SMRV\\1811.html:text/html}
}

@inproceedings{chakraborty_interpretability_2017,
	address = {San Francisco, CA},
	title = {Interpretability of deep learning models: {A} survey of results},
	isbn = {978-1-5386-0435-9},
	shorttitle = {Interpretability of deep learning models},
	url = {https://ieeexplore.ieee.org/document/8397411/},
	doi = {10.1109/UIC-ATC.2017.8397411},
	urldate = {2020-03-22},
	booktitle = {2017 {IEEE} {SmartWorld}, {Ubiquitous} {Intelligence} \& {Computing}, {Advanced} \& {Trusted} {Computed}, {Scalable} {Computing} \& {Communications}, {Cloud} \& {Big} {Data} {Computing}, {Internet} of {People} and {Smart} {City} {Innovation} ({SmartWorld}/{SCALCOM}/{UIC}/{ATC}/{CBDCom}/{IOP}/{SCI})},
	publisher = {IEEE},
	author = {Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and Kelley, Troy D. and Braines, Dave and Sensoy, Murat and Willis, Christopher J. and Gurram, Prudhvi},
	month = aug,
	year = {2017},
	pages = {1--6},
	file = {Accepted Version:C\:\\Users\\13762012\\Zotero\\storage\\PFX4P7V9\\Chakraborty et al. - 2017 - Interpretability of deep learning models A survey.pdf:application/pdf}
}

@inproceedings{caruana_intelligible_2015,
	address = {Sydney, NSW, Australia},
	title = {Intelligible {Models} for {HealthCare}: {Predicting} {Pneumonia} {Risk} and {Hospital} 30-day {Readmission}},
	isbn = {978-1-4503-3664-2},
	shorttitle = {Intelligible {Models} for {HealthCare}},
	url = {http://dl.acm.org/citation.cfm?doid=2783258.2788613},
	doi = {10.1145/2783258.2788613},
	language = {en},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the 21th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining} - {KDD} '15},
	publisher = {ACM Press},
	author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
	year = {2015},
	pages = {1721--1730},
	file = {Caruana et al. - 2015 - Intelligible Models for HealthCare Predicting Pne.pdf:C\:\\Users\\13762012\\Zotero\\storage\\T9NCQHK4\\Caruana et al. - 2015 - Intelligible Models for HealthCare Predicting Pne.pdf:application/pdf}
}

@inproceedings{russell_efficient_2019,
	address = {Atlanta, GA, USA},
	title = {Efficient {Search} for {Diverse} {Coherent} {Explanations}},
	isbn = {978-1-4503-6125-5},
	url = {http://dl.acm.org/citation.cfm?doid=3287560.3287569},
	doi = {10.1145/3287560.3287569},
	language = {en},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the {Conference} on {Fairness}, {Accountability}, and {Transparency} - {FAT}* '19},
	publisher = {ACM Press},
	author = {Russell, Chris},
	year = {2019},
	pages = {20--28},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\A6ZEIGL4\\Russell - 2019 - Efficient Search for Diverse Coherent Explanations.pdf:application/pdf}
}

@inproceedings{garg_counterfactual_2019,
	address = {Honolulu HI USA},
	title = {Counterfactual {Fairness} in {Text} {Classification} through {Robustness}},
	isbn = {978-1-4503-6324-2},
	url = {https://dl.acm.org/doi/10.1145/3306618.3317950},
	doi = {10.1145/3306618.3317950},
	language = {en},
	urldate = {2020-03-23},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Garg, Sahaj and Perot, Vincent and Limtiaco, Nicole and Taly, Ankur and Chi, Ed H. and Beutel, Alex},
	month = jan,
	year = {2019},
	pages = {219--226},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\ENCDIWPE\\Garg et al. - 2019 - Counterfactual Fairness in Text Classification thr.pdf:application/pdf;Full Text:C\:\\Users\\13762012\\Zotero\\storage\\Y5WYSQM6\\Garg et al. - 2019 - Counterfactual Fairness in Text Classification thr.pdf:application/pdf}
}

@incollection{ghorbani_towards_2019-1,
	title = {Towards {Automatic} {Concept}-based {Explanations}},
	url = {http://papers.nips.cc/paper/9126-towards-automatic-concept-based-explanations.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {9277--9286}
}

@incollection{kim_learning_2019-1,
	title = {Learning {Dynamics} of {Attention}: {Human} {Prior} for {Interpretable} {Machine} {Reasoning}},
	url = {http://papers.nips.cc/paper/8835-learning-dynamics-of-attention-human-prior-for-interpretable-machine-reasoning.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Kim, Wonjae and Lee, Yoonho},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {6021--6032}
}

@article{goyal_explaining_2020,
	title = {Explaining {Classifiers} with {Causal} {Concept} {Effect} ({CaCE})},
	url = {http://arxiv.org/abs/1907.07165},
	abstract = {How can we understand classification decisions made by deep neural networks? Many existing explainability methods rely solely on correlations and fail to account for confounding, which may result in potentially misleading explanations. To overcome this problem, we define the Causal Concept Effect (CaCE) as the causal effect of (the presence or absence of) a human-interpretable concept on a deep neural net's predictions. We show that the CaCE measure can avoid errors stemming from confounding. Estimating CaCE is difficult in situations where we cannot easily simulate the do-operator. To mitigate this problem, we use a generative model, specifically a Variational AutoEncoder (VAE), to measure VAE-CaCE. In an extensive experimental analysis, we show that the VAE-CaCE is able to estimate the true concept causal effect, compared to baselines for a number of datasets including high dimensional images.},
	urldate = {2020-03-24},
	journal = {arXiv:1907.07165 [cs, stat]},
	author = {Goyal, Yash and Feder, Amir and Shalit, Uri and Kim, Been},
	month = feb,
	year = {2020},
	note = {arXiv: 1907.07165},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\K2ENL8XS\\Goyal et al. - 2020 - Explaining Classifiers with Causal Concept Effect .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\2Q8MZNTE\\1907.html:text/html}
}

@article{liu_generative_2019,
	title = {Generative {Counterfactual} {Introspection} for {Explainable} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1907.03077},
	abstract = {In this work, we propose an introspection technique for deep neural networks that relies on a generative model to instigate salient editing of the input image for model interpretation. Such modification provides the fundamental interventional operation that allows us to obtain answers to counterfactual inquiries, i.e., what meaningful change can be made to the input image in order to alter the prediction. We demonstrate how to reveal interesting properties of the given classifiers by utilizing the proposed introspection approach on both the MNIST and the CelebA dataset.},
	urldate = {2020-03-24},
	journal = {arXiv:1907.03077 [cs, stat]},
	author = {Liu, Shusen and Kailkhura, Bhavya and Loveland, Donald and Han, Yong},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03077},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\QNHRFGUN\\Liu et al. - 2019 - Generative Counterfactual Introspection for Explai.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\KBZ7QRPY\\1907.html:text/html}
}

@article{bertsimas_price_2019,
	title = {The {Price} of {Interpretability}},
	url = {http://arxiv.org/abs/1907.03419},
	abstract = {When quantitative models are used to support decision-making on complex and important topics, understanding a model's ``reasoning'' can increase trust in its predictions, expose hidden biases, or reduce vulnerability to adversarial attacks. However, the concept of interpretability remains loosely defined and application-specific. In this paper, we introduce a mathematical framework in which machine learning models are constructed in a sequence of interpretable steps. We show that for a variety of models, a natural choice of interpretable steps recovers standard interpretability proxies (e.g., sparsity in linear models). We then generalize these proxies to yield a parametrized family of consistent measures of model interpretability. This formal definition allows us to quantify the ``price'' of interpretability, i.e., the tradeoff with predictive accuracy. We demonstrate practical algorithms to apply our framework on real and synthetic datasets.},
	urldate = {2020-03-24},
	journal = {arXiv:1907.03419 [cs, stat]},
	author = {Bertsimas, Dimitris and Delarue, Arthur and Jaillet, Patrick and Martin, Sebastien},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03419},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\CN4C9D8M\\Bertsimas et al. - 2019 - The Price of Interpretability.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\FXP65GL3\\1907.html:text/html}
}

@article{miller_explanation_2019-1,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {00043702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	language = {en},
	urldate = {2020-03-24},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	pages = {1--38},
	file = {Submitted Version:C\:\\Users\\13762012\\Zotero\\storage\\Z9KSK965\\Miller - 2019 - Explanation in artificial intelligence Insights f.pdf:application/pdf}
}

@article{du_techniques_2019-1,
	title = {Techniques for {Interpretable} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1808.00033},
	abstract = {Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.},
	urldate = {2020-03-24},
	journal = {arXiv:1808.00033 [cs, stat]},
	author = {Du, Mengnan and Liu, Ninghao and Hu, Xia},
	month = may,
	year = {2019},
	note = {arXiv: 1808.00033},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted by Communications of the ACM (CACM), Review Article},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\7K739CW4\\Du et al. - 2019 - Techniques for Interpretable Machine Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\IA2B4AWD\\1808.html:text/html}
}

@article{arrieta_explainable_2019-1,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, {Taxonomies}, {Opportunities} and {Challenges} toward {Responsible} {AI}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {http://arxiv.org/abs/1910.10045},
	abstract = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	urldate = {2020-03-24},
	journal = {arXiv:1910.10045 [cs]},
	author = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = dec,
	year = {2019},
	note = {arXiv: 1910.10045},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 67 pages, 13 figures, accepted for its publication in Information Fusion},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\9WXST45R\\Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\26Z9E5J4\\1910.html:text/html}
}

@article{lage_evaluation_2019,
	title = {An {Evaluation} of the {Human}-{Interpretability} of {Explanation}},
	url = {http://arxiv.org/abs/1902.00006},
	abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable under three specific tasks that users may perform with machine learning systems: simulation of the response, verification of a suggested response, and determining whether the correctness of a suggested response changes under a change to the inputs. Through carefully controlled human-subject experiments, we identify regularizers that can be used to optimize for the interpretability of machine learning systems. Our results show that the type of complexity matters: cognitive chunks (newly defined concepts) affect performance more than variable repetitions, and these trends are consistent across tasks and domains. This suggests that there may exist some common design principles for explanation systems.},
	urldate = {2020-03-24},
	journal = {arXiv:1902.00006 [cs, stat]},
	author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
	month = aug,
	year = {2019},
	note = {arXiv: 1902.00006},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1802.00682},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\UM6J3JJG\\Lage et al. - 2019 - An Evaluation of the Human-Interpretability of Exp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\DN5RQZ29\\1902.html:text/html}
}

@inproceedings{ghazimatin_prince_2020,
	address = {Houston TX USA},
	title = {{PRINCE}: {Provider}-side {Interpretability} with {Counterfactual} {Explanations} in {Recommender} {Systems}},
	isbn = {978-1-4503-6822-3},
	shorttitle = {{PRINCE}},
	url = {https://dl.acm.org/doi/10.1145/3336191.3371824},
	doi = {10.1145/3336191.3371824},
	language = {en},
	urldate = {2020-03-24},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ghazimatin, Azin and Balalau, Oana and Saha Roy, Rishiraj and Weikum, Gerhard},
	month = jan,
	year = {2020},
	pages = {196--204},
	file = {Submitted Version:C\:\\Users\\13762012\\Zotero\\storage\\4J3G32RM\\Ghazimatin et al. - 2020 - PRINCE Provider-side Interpretability with Counte.pdf:application/pdf}
}

@inproceedings{gilpin_explaining_2018,
	address = {Turin, Italy},
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	isbn = {978-1-5386-5090-5},
	shorttitle = {Explaining {Explanations}},
	url = {https://ieeexplore.ieee.org/document/8631448/},
	doi = {10.1109/DSAA.2018.00018},
	urldate = {2020-03-24},
	booktitle = {2018 {IEEE} 5th {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	publisher = {IEEE},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month = oct,
	year = {2018},
	pages = {80--89},
	file = {Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf:C\:\\Users\\13762012\\Zotero\\storage\\PICCAR57\\Gilpin et al. - 2018 - Explaining Explanations An Overview of Interpreta.pdf:application/pdf}
}

@article{lakkaraju_interpretable_2017,
	title = {Interpretable \& {Explorable} {Approximations} of {Black} {Box} {Models}},
	url = {http://arxiv.org/abs/1707.01154},
	abstract = {We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation. To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space. Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user. To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences. Experimental evaluation with real-world datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.},
	urldate = {2020-03-24},
	journal = {arXiv:1707.01154 [cs]},
	author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01154},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Presented as a poster at the 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\EKYJLXZG\\Lakkaraju et al. - 2017 - Interpretable & Explorable Approximations of Black.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\G5HDPIAB\\1707.html:text/html}
}

@article{guidotti_survey_2018,
	title = {A {Survey} of {Methods} for {Explaining} {Black} {Box} {Models}},
	volume = {51},
	issn = {03600300},
	url = {http://dl.acm.org/citation.cfm?doid=3271482.3236009},
	doi = {10.1145/3236009},
	language = {en},
	number = {5},
	urldate = {2020-03-24},
	journal = {ACM Computing Surveys},
	author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
	month = aug,
	year = {2018},
	pages = {1--42},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\KH9DRTG6\\Guidotti et al. - 2018 - A Survey of Methods for Explaining Black Box Model.pdf:application/pdf}
}

@inproceedings{ghazimatin_prince_2020-1,
	address = {Houston TX USA},
	title = {{PRINCE}: {Provider}-side {Interpretability} with {Counterfactual} {Explanations} in {Recommender} {Systems}},
	isbn = {978-1-4503-6822-3},
	shorttitle = {{PRINCE}},
	url = {https://dl.acm.org/doi/10.1145/3336191.3371824},
	doi = {10.1145/3336191.3371824},
	language = {en},
	urldate = {2020-03-24},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ghazimatin, Azin and Balalau, Oana and Saha Roy, Rishiraj and Weikum, Gerhard},
	month = jan,
	year = {2020},
	pages = {196--204},
	file = {Submitted Version:C\:\\Users\\13762012\\Zotero\\storage\\N3FNEUXD\\Ghazimatin et al. - 2020 - PRINCE Provider-side Interpretability with Counte.pdf:application/pdf}
}

@article{rudin_stop_2019,
	title = {Stop {Explaining} {Black} {Box} {Machine} {Learning} {Models} for {High} {Stakes} {Decisions} and {Use} {Interpretable} {Models} {Instead}},
	url = {http://arxiv.org/abs/1811.10154},
	abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to {\textbackslash}textit\{explain\} black box models, rather than creating models that are {\textbackslash}textit\{interpretable\} in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward -- it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
	urldate = {2020-03-25},
	journal = {arXiv:1811.10154 [cs, stat]},
	author = {Rudin, Cynthia},
	month = sep,
	year = {2019},
	note = {arXiv: 1811.10154},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Author's pre-publication version of a 2019 Nature Machine Intelligence article. Shorter Version was published in NIPS 2018 Workshop on Critiquing and Correcting Trends in Machine Learning. Expands also on NSF Statistics at a Crossroads Webinar},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\2LEBFXL5\\Rudin - 2019 - Stop Explaining Black Box Machine Learning Models .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\CT7LD9YE\\1811.html:text/html;Full Text:C\:\\Users\\13762012\\Zotero\\storage\\8NU7JKLE\\Rudin - 2019 - Stop Explaining Black Box Machine Learning Models .pdf:application/pdf}
}

@article{hendricks2018generating,
  title={Generating counterfactual explanations with natural language},
  author={Hendricks, Lisa Anne and Hu, Ronghang and Darrell, Trevor and Akata, Zeynep},
  journal={arXiv preprint arXiv:1806.09809},
  year={2018}
}

@article{artelt2020convex,
  title={Convex Density Constraints for Computing Plausible Counterfactual Explanations},
  author={Artelt, Andr{\'e} and Hammer, Barbara},
  journal={arXiv preprint arXiv:2002.04862},
  year={2020}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in neural information processing systems},
  pages={4765--4774},
  year={2017}
}

@inproceedings{dhurandhar2018explanations,
  title={Explanations based on the missing: Towards contrastive explanations with pertinent negatives},
  author={Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
  booktitle={Advances in neural information processing systems},
  pages={592--603},
  year={2018}
}

@article{sharma2019certifai,
  title={Certifai: Counterfactual explanations for robustness, transparency, interpretability, and fairness of artificial intelligence models},
  author={Sharma, Shubham and Henderson, Jette and Ghosh, Joydeep},
  journal={arXiv preprint arXiv:1905.07857},
  year={2019}
}


@article{chattopadhyay2019neural,
  title={Neural network attributions: A causal perspective},
  author={Chattopadhyay, Aditya and Manupriya, Piyushi and Sarkar, Anirban and Balasubramanian, Vineeth N},
  journal={arXiv preprint arXiv:1902.02302},
  year={2019}
}

@article{parafita2019explaining,
  title={Explaining Visual Models by Causal Attribution},
  author={Parafita, {\'A}lvaro and Vitri{\`a}, Jordi},
  journal={arXiv preprint arXiv:1909.08891},
  year={2019}
}


@article{narendra2018explaining,
  title={Explaining deep learning models using causal inference},
  author={Narendra, Tanmayee and Sankaran, Anush and Vijaykeerthy, Deepak and Mani, Senthil},
  journal={arXiv preprint arXiv:1811.04376},
  year={2018}
}



@article{alvarez2017causal,
  title={A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  journal={arXiv preprint arXiv:1707.01943},
  year={2017}
}

@inproceedings{zhang2019axiomatic,
  title={Axiomatic Interpretability for Multiclass Additive Models},
  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={226--234},
  year={2019}
}

@article{darwen2019bayesian,
  title={Bayesian model averaging for river flow prediction},
  author={Darwen, Paul J},
  journal={Applied Intelligence},
  volume={49},
  number={1},
  pages={103--111},
  year={2019},
  publisher={Springer}
}

@article{deng2019interpreting,
  title={Interpreting tree ensembles with intrees},
  author={Deng, Houtao},
  journal={International Journal of Data Science and Analytics},
  volume={7},
  number={4},
  pages={277--287},
  year={2019},
  publisher={Springer}
}




@article{letham2015interpretable,
  title={Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model},
  author={Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H and Madigan, David and others},
  journal={The Annals of Applied Statistics},
  volume={9},
  number={3},
  pages={1350--1371},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1885--1894},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{russell2019efficient,
  title={Efficient search for diverse coherent explanations},
  author={Russell, Chris},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={20--28},
  year={2019}
}


@article{van2019interpretable,
  title={Interpretable counterfactual explanations guided by prototypes},
  author={Van Looveren, Arnaud and Klaise, Janis},
  journal={arXiv preprint arXiv:1907.02584},
  year={2019}
}


@article{krishnan1999extracting,
  title={Extracting decision trees from trained neural networks},
  author={Krishnan, R and Sivakumar, G and Bhattacharya, P},
  journal={Pattern recognition},
  volume={32},
  number={12},
  year={1999},
  publisher={Elsevier}
}


@inproceedings{boz2002extracting,
  title={Extracting decision trees from trained neural networks},
  author={Boz, Olcay},
  booktitle={Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={456--461},
  year={2002}
}

@article{guidotti2018local,
  title={Local rule-based explanations of black box decision systems},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  journal={arXiv preprint arXiv:1805.10820},
  year={2018}
}


@inproceedings{hendricks2016generating,
  title={Generating visual explanations},
  author={Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
  booktitle={European Conference on Computer Vision},
  pages={3--19},
  year={2016},
  organization={Springer}
}

@article{harradon2018causal,
  title={Causal learning and explanation of deep neural networks via autoencoded activations},
  author={Harradon, Michael and Druce, Jeff and Ruttenberg, Brian},
  journal={arXiv preprint arXiv:1802.00541},
  year={2018}
}

@inproceedings{schwab2019granger,
  title={Granger-causal attentive mixtures of experts: Learning important features with neural networks},
  author={Schwab, Patrick and Miladinovic, Djordje and Karlen, Walter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={4846--4853},
  year={2019}
}




@article{wang_multi-value_2017,
	title = {Multi-{Value} {Rule} {Sets}},
	url = {http://arxiv.org/abs/1710.05257},
	abstract = {We present the Multi-vAlue Rule Set (MARS) model for interpretable classification with feature efficient presentations. MARS introduces a more generalized form of association rules that allows multiple values in a condition. Rules of this form are more concise than traditional single-valued rules in capturing and describing patterns in data. MARS mitigates the problem of dealing with continuous features and high-cardinality categorical features faced by rule-based models. Our formulation also pursues a higher efficiency of feature utilization, which reduces the cognitive load to understand the decision process. We propose an efficient inference method for learning a maximum a posteriori model, incorporating theoretically grounded bounds to iteratively reduce the search space to improve search efficiency. Experiments with synthetic and real-world data demonstrate that MARS models have significantly smaller complexity and fewer features, providing better interpretability while being competitive in predictive accuracy. We conducted a usability study with human subjects and results show that MARS is the easiest to use compared with other competing rule-based models, in terms of the correct rate and response time. Overall, MARS introduces a new approach to rule-based models that balance accuracy and interpretability with feature-efficient representations.},
	urldate = {2020-03-11},
	journal = {arXiv:1710.05257 [cs], NIPS},
	author = {Wang, Tong},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.05257},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\S8TCDR7Z\\Wang - 2017 - Multi-Value Rule Sets.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\GYVHS9II\\1710.html:text/html}
}

@article{letham_interpretable_2015,
	title = {Interpretable classifiers using rules and {Bayesian} analysis: {Building} a better stroke prediction model},
	volume = {9},
	issn = {1932-6157},
	shorttitle = {Interpretable classifiers using rules and {Bayesian} analysis},
	url = {http://arxiv.org/abs/1511.01644},
	doi = {10.1214/15-AOAS848},
	abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if...then... statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS\$\_2\$ score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS\$\_2\$, but more accurate.},
	number = {3},
	urldate = {2020-03-13},
	journal = {The Annals of Applied Statistics},
	author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
	month = sep,
	year = {2015},
	note = {arXiv: 1511.01644},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Applications},
	pages = {1350--1371},
	annote = {Comment: Published at http://dx.doi.org/10.1214/15-AOAS848 in the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv Fulltext PDF:C\:\\Users\\13762012\\Zotero\\storage\\RD6N2B2X\\Letham et al. - 2015 - Interpretable classifiers using rules and Bayesian.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\13762012\\Zotero\\storage\\PJ936NQ7\\1511.html:text/html}
}


@article{schwab_granger-causal_2019,
	title = {Granger-{Causal} {Attentive} {Mixtures} of {Experts}: {Learning} {Important} {Features} with {Neural} {Networks}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	shorttitle = {Granger-{Causal} {Attentive} {Mixtures} of {Experts}},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4412},
	doi = {10.1609/aaai.v33i01.33014846},
	abstract = {Knowledge of the importance of input features towards decisions made by machine-learning models is essential to increase our understanding of both the models and the underlying data. Here, we present a new approach to estimating feature importance with neural networks based on the idea of distributing the features of interest among experts in an attentive mixture of experts (AME). AMEs use attentive gating networks trained with a Granger-causal objective to learn to jointly produce accurate predictions as well as estimates of feature importance in a single model. Our experiments show (i) that the feature importance estimates provided by AMEs compare favourably to those provided by state-of-theart methods, (ii) that AMEs are significantly faster at estimating feature importance than existing methods, and (iii) that the associations discovered by AMEs are consistent with those reported by domain experts.},
	urldate = {2020-04-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Schwab, Patrick and Miladinovic, Djordje and Karlen, Walter},
	month = jul,
	year = {2019},
	pages = {4846--4853},
	file = {Full Text:C\:\\Users\\13762012\\Zotero\\storage\\TW8HJXBB\\Schwab et al. - 2019 - Granger-Causal Attentive Mixtures of Experts Lear.pdf:application/pdf}
}


@article{lipovetsky2001analysis,
  title={Analysis of regression in game theory approach},
  author={Lipovetsky, Stan and Conklin, Michael},
  journal={Applied Stochastic Models in Business and Industry},
  volume={17},
  number={4},
  pages={319--330},
  year={2001},
  publisher={Wiley Online Library}
}

@article{vstrumbelj2014explaining,
  title={Explaining prediction models and individual predictions with feature contributions},
  author={{\v{S}}trumbelj, Erik and Kononenko, Igor},
  journal={Knowledge and information systems},
  volume={41},
  number={3},
  pages={647--665},
  year={2014},
  publisher={Springer}
}

@inproceedings{datta2016algorithmic,
  title={Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems},
  author={Datta, Anupam and Sen, Shayak and Zick, Yair},
  booktitle={2016 IEEE symposium on security and privacy (SP)},
  pages={598--617},
  year={2016},
  organization={IEEE}
}


@article{quinlan1987simplifying,
  title={Simplifying decision trees},
  author={Quinlan, J Ross},
  journal={International journal of man-machine studies},
  volume={27},
  number={3},
  pages={221--234},
  year={1987},
  publisher={Elsevier}
}

@inproceedings{guo2018explaining,
  title={Explaining Deep Learning Models--A Bayesian Non-parametric Approach},
  author={Guo, Wenbo and Huang, Sui and Tao, Yunzhe and Xing, Xinyu and Lin, Lin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4514--4524},
  year={2018}
}

@article{krishnan1999extracting,
  title={Extracting decision trees from trained neural networks},
  author={Krishnan, R and Sivakumar, G and Bhattacharya, P},
  journal={Pattern recognition},
  volume={32},
  number={12},
  year={1999},
  publisher={Elsevier}
}

@inproceedings{tan2018distill,
  title={Distill-and-compare: Auditing black-box models using transparent model distillation},
  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={303--310},
  year={2018}
}

@article{altmann2010permutation,
  title={Permutation importance: a corrected feature importance measure},
  author={Altmann, Andr{\'e} and Tolo{\c{s}}i, Laura and Sander, Oliver and Lengauer, Thomas},
  journal={Bioinformatics},
  volume={26},
  number={10},
  pages={1340--1347},
  year={2010},
  publisher={Oxford University Press}
}

@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}

@inproceedings{sundararajan2017axiomatic,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3319--3328},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3145--3153},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  booktitle={Advances in neural information processing systems},
  pages={4765--4774},
  year={2017}
}

@article{guidotti2019factual,
  title={Factual and Counterfactual Explanations for Black Box Decision Making},
  author={Guidotti, Riccardo and Monreale, Anna and Giannotti, Fosca and Pedreschi, Dino and Ruggieri, Salvatore and Turini, Franco},
  journal={IEEE Intelligent Systems},
  year={2019},
  publisher={IEEE}
}

@inproceedings{lakkaraju2016interpretable,
  title={Interpretable decision sets: A joint framework for description and prediction},
  author={Lakkaraju, Himabindu and Bach, Stephen H and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1675--1684},
  year={2016}
}

@article{jang1993functional,
  title={Functional equivalence between radial basis function networks and fuzzy inference systems},
  author={Jang, J-SR and Sun, C-T},
  journal={IEEE transactions on Neural Networks},
  volume={4},
  number={1},
  pages={156--159},
  year={1993},
  publisher={IEEE}
}

@article{guillaume2001designing,
  title={Designing fuzzy inference systems from data: An interpretability-oriented review},
  author={Guillaume, Serge},
  journal={IEEE Transactions on fuzzy systems},
  volume={9},
  number={3},
  pages={426--443},
  year={2001},
  publisher={IEEE}
}

@article{wang2002self,
  title={Self-adaptive neuro-fuzzy inference systems for classification applications},
  author={Wang, Jeen-Shing and Lee, CS George},
  journal={IEEE Transactions on Fuzzy Systems},
  volume={10},
  number={6},
  pages={790--802},
  year={2002},
  publisher={IEEE}
}


@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}



@Article{electronics8080832,
AUTHOR = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
TITLE = {Machine Learning Interpretability: A Survey on Methods and Metrics},
JOURNAL = {Electronics},
VOLUME = {8},
YEAR = {2019},
NUMBER = {8},
ARTICLE-NUMBER = {832},
URL = {https://www.mdpi.com/2079-9292/8/8/832},
ISSN = {2079-9292},
ABSTRACT = {Machine learning systems are becoming increasingly ubiquitous. These systems&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
DOI = {10.3390/electronics8080832}
}

@article{RN76,
   author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
   title = {A Survey of Methods for Explaining Black Box Models},
   journal = {ACM Computing Surveys},
   volume = {51},
   number = {5},
   pages = {1-42},
   ISSN = {03600300},
   DOI = {10.1145/3236009},
   url = {http://dl.acm.org/citation.cfm?doid=3271482.3236009
https://dl.acm.org/ft_gateway.cfm?id=3236009&type=pdf},
   year = {2018},
   type = {Journal Article}
}


@article{gunning2017explainable,
  title={Explainable artificial intelligence (xai)},
  author={Gunning, David},
  journal={Defense Advanced Research Projects Agency (DARPA), nd Web},
  volume={2},
  year={2017}
}

@inproceedings{Kim2017InterpretabilityBF,
  title={Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)},
  author={Been Kim and Martin Wattenberg and Justin Gilmer and Carrie J. Cai and James Wexler and Fernanda B. Vi{\'e}gas and Rory Sayres},
  booktitle={ICML},
  year={2017}
}



@inproceedings{10.1145/2876034.2876042,
author = {Williams, Joseph Jay and Kim, Juho and Rafferty, Anna and Maldonado, Samuel and Gajos, Krzysztof Z. and Lasecki, Walter S. and Heffernan, Neil},
title = {AXIS: Generating Explanations at Scale with Learnersourcing and Machine Learning},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2876042},
doi = {10.1145/2876034.2876042},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {379–388},
numpages = {10},
keywords = {machine learning, explanation, learnersourcing, adaptive learning, learning at scale, crowdsourcing},
location = {Edinburgh, Scotland, UK},
series = {L@S ’16}
}

@inproceedings{10.5555/2984093.2984126,
author = {Chang, Jonathan and Boyd-Graber, Jordan and Gerrish, Sean and Wang, Chong and Blei, David M.},
title = {Reading Tea Leaves: How Humans Interpret Topic Models},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {288–296},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS’09}
}
  
@ARTICLE{2017arXiv171111279K,
       author = {{Kim}, Been and {Wattenberg}, Martin and {Gilmer}, Justin and
         {Cai}, Carrie and {Wexler}, James and {Viegas}, Fernanda and
         {Sayres}, Rory},
        title = "{Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning},
         year = 2017,
        month = nov,
          eid = {arXiv:1711.11279},
        pages = {arXiv:1711.11279},
archivePrefix = {arXiv},
       eprint = {1711.11279},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171111279K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{RN77,
   author = {Rudin, Cynthia},
   title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
   journal = {arXiv:1811.10154 [cs, stat]},
   url = {http://arxiv.org/abs/1811.10154
https://arxiv.org/abs/1811.10154},
   year = {2019},
   type = {Journal Article}
}


@article{pearl2019seven,
  title={The seven tools of causal inference, with reflections on machine learning},
  author={Pearl, Judea},
  journal={Communications of the ACM},
  volume={62},
  number={3},
  pages={54--60},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@misc{dowhy,
authors={Sharma, Amit and Kiciman, Emre and others},
title={Do{W}hy: {A Python package for causal inference}},
howpublished={https://github.com/microsoft/dowhy}
year={2019}
}

@misc{chen2020causalml, title={CausalML: Python Package for Causal Machine Learning}, author={Huigang Chen and Totte Harinen and Jeong-Yoon Lee and Mike Yung and Zhenyu Zhao}, year={2020}, eprint={2002.11631}, archivePrefix={arXiv}, primaryClass={cs.CY} }

@misc{econml,
  author={Microsoft Research},
  title={{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},
  howpublished={https://github.com/microsoft/EconML},
  note={Version 0.x},
  year={2019}
}
@ARTICLE{2019arXiv190302278K,
       author = {{Kalainathan}, Diviyan and {Goudet}, Olivier},
        title = "{Causal Discovery Toolbox: Uncover causal relationships in Python}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Computation, Statistics - Machine Learning},
         year = 2019,
        month = mar,
          eid = {arXiv:1903.02278},
        pages = {arXiv:1903.02278},
archivePrefix = {arXiv},
       eprint = {1903.02278},
 primaryClass = {stat.CO},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190302278K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{RN165,
   author = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
   title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
   journal = {Proceedings of the National Academy of Sciences},
   volume = {116},
   pages = {4156-4165},
   year = {2019},
   type = {Journal Article}
}

@misc{RN199,
   pages = {arXiv:1608.00060},
   month = {July 01, 2016},
   url = {https://ui.adsabs.harvard.edu/abs/2016arXiv160800060C},
   year = {2016},
   type = {Electronic Article}
}

@article{chernozhukov2016double,
  title={Double/debiased machine learning for treatment and causal parameters},
  author={Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  journal={arXiv preprint arXiv:1608.00060},
  year={2016}
}


@article{oprescu2018orthogonal,
  title={Orthogonal random forest for causal inference},
  author={Oprescu, Miruna and Syrgkanis, Vasilis and Wu, Zhiwei Steven},
  journal={arXiv preprint arXiv:1806.03467},
  year={2018}
}

@article{foster2019orthogonal,
  title={Orthogonal statistical learning},
  author={Foster, Dylan J and Syrgkanis, Vasilis},
  journal={arXiv preprint arXiv:1901.09036},
  year={2019}
}

@book{10.5555/3238230,
author = {Pearl, Judea and Mackenzie, Dana},
title = {The Book of Why: The New Science of Cause and Effect},
year = {2018},
isbn = {046509760X},
publisher = {Basic Books, Inc.},
address = {USA},
edition = {1st}
}


@article {Rungeeaau4996,
	author = {Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
	title = {Detecting and quantifying causal associations in large nonlinear time series datasets},
	volume = {5},
	number = {11},
	elocation-id = {eaau4996},
	year = {2019},
	doi = {10.1126/sciadv.aau4996},
	publisher = {American Association for the Advancement of Science},
	abstract = {Identifying causal relationships and quantifying their strength from observational time series data are key problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body. Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets. We validate the method on time series of well-understood physical mechanisms in the climate system and the human heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely new possibilities to discover and quantify causal networks from time series across a range of research fields.},
	URL = {https://advances.sciencemag.org/content/5/11/eaau4996},
	eprint = {https://advances.sciencemag.org/content/5/11/eaau4996.full.pdf},
	journal = {Science Advances}
}


@article{doi:10.1063/1.5025050,
author = {Runge,J. },
title = {Causal network reconstruction from time series: From theoretical assumptions to practical estimation},
journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
volume = {28},
number = {7},
pages = {075310},
year = {2018},
doi = {10.1063/1.5025050},

URL = { 
        https://doi.org/10.1063/1.5025050
    
},
eprint = { 
        https://doi.org/10.1063/1.5025050
    
}}


@article{runge2015identifying,
  title={Identifying causal gateways and mediators in complex spatio-temporal systems},
  author={Runge, Jakob and Petoukhov, Vladimir and Donges, Jonathan F and Hlinka, Jaroslav and Jajcay, Nikola and Vejmelka, Martin and Hartman, David and Marwan, Norbert and Palu{\v{s}}, Milan and Kurths, J{\"u}rgen},
  journal={Nature communications},
  volume={6},
  number={1},
  pages={1--10},
  year={2015},
  publisher={Nature Publishing Group}
}


@article{runge2015quantifying,
  title={Quantifying information transfer and mediation along causal pathways in complex systems},
  author={Runge, Jakob},
  journal={Physical Review E},
  volume={92},
  number={6},
  pages={062829},
  year={2015},
  publisher={APS}
}

@article{runge2017conditional,
  title={Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
  author={Runge, Jakob},
  journal={arXiv preprint arXiv:1709.01447},
  year={2017}
}

@ARTICLE{2017arXiv170508492Z,
       author = {{Zhao}, Yan and {Fang}, Xiao and {Simchi-Levi}, David},
        title = "{Uplift Modeling with Multiple Treatments and General Response Types}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence},
         year = 2017,
        month = may,
          eid = {arXiv:1705.08492},
        pages = {arXiv:1705.08492},
archivePrefix = {arXiv},
       eprint = {1705.08492},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170508492Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{radcliffe2011real,
  title={Real-world uplift modelling with significance-based uplift trees},
  author={Radcliffe, Nicholas J and Surry, Patrick D},
  journal={White Paper TR-2011-1, Stochastic Solutions},
  pages={1--33},
  year={2011},
  publisher={Citeseer}
}

@inproceedings{xian2019reinforcement,
  title={Reinforcement knowledge graph reasoning for explainable recommendation},
  author={Xian, Yikun and Fu, Zuohui and Muthukrishnan, S and De Melo, Gerard and Zhang, Yongfeng},
  booktitle={Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={285--294},
  year={2019}
}

@article{dehejia2002propensity,
  title={Propensity score-matching methods for nonexperimental causal studies},
  author={Dehejia, Rajeev H and Wahba, Sadek},
  journal={Review of Economics and statistics},
  volume={84},
  number={1},
  pages={151--161},
  year={2002},
  publisher={MIT Press}
}

@article{frangakis2002principal,
  title={Principal stratification in causal inference},
  author={Frangakis, Constantine E and Rubin, Donald B},
  journal={Biometrics},
  volume={58},
  number={1},
  pages={21--29},
  year={2002},
  publisher={Wiley Online Library}
}

@inproceedings{ustun2019actionable,
  title={Actionable recourse in linear classification},
  author={Ustun, Berk and Spangher, Alexander and Liu, Yang},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={10--19},
  year={2019}
}

@inproceedings{poyiadzi2020face,
  title={FACE: Feasible and actionable counterfactual explanations},
  author={Poyiadzi, Rafael and Sokol, Kacper and Santos-Rodriguez, Raul and De Bie, Tijl and Flach, Peter},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={344--350},
  year={2020}
}




@article{article_causal,
author = {Pearl, Judea},
year = {2009},
month = {01},
pages = {96-146},
title = {Causal Inference in Statistics: An Overview},
volume = {3},
journal = {Statistics Surveys},
doi = {10.1214/09-SS057}
}

  
@article{bau2018gan,
  title={Gan dissection: Visualizing and understanding generative adversarial networks},
  author={Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B and Freeman, William T and Torralba, Antonio},
  journal={arXiv preprint arXiv:1811.10597},
  year={2018}
}

@article{madumal2019explainable,
  title={Explainable reinforcement learning through a causal lens},
  author={Madumal, Prashan and Miller, Tim and Sonenberg, Liz and Vetere, Frank},
  journal={arXiv preprint arXiv:1905.10958},
  year={2019}
}

@article{alvarez2017causal,
  title={A causal framework for explaining the predictions of black-box sequence-to-sequence models},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  journal={arXiv preprint arXiv:1707.01943},
  year={2017}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{goldstein2015peeking,
  title={Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation},
  author={Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
  journal={Journal of Computational and Graphical Statistics},
  volume={24},
  number={1},
  pages={44--65},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{zhao2019causal,
  title={Causal interpretations of black-box models},
  author={Zhao, Qingyuan and Hastie, Trevor},
  journal={Journal of Business \& Economic Statistics},
  pages={1--10},
  year={2019},
  publisher={Taylor \& Francis}
}


@article{doi:10.1080/07350015.2019.1624293,
author = {Qingyuan Zhao and Trevor Hastie},
title = {Causal Interpretations of Black-Box Models},
journal = {Journal of Business \& Economic Statistics},
volume = {0},
number = {0},
pages = {1-10},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2019.1624293},

URL = { 
        https://doi.org/10.1080/07350015.2019.1624293
    
},
eprint = { 
        https://doi.org/10.1080/07350015.2019.1624293
    
}}


@ARTICLE{2019arXiv190108162D,
       author = {{Dasgupta}, Ishita and {Wang}, Jane and {Chiappa}, Silvia and
         {Mitrovic}, Jovana and {Ortega}, Pedro and {Raposo}, David and
         {Hughes}, Edward and {Battaglia}, Peter and {Botvinick}, Matthew and
         {Kurth-Nelson}, Zeb},
        title = "{Causal Reasoning from Meta-reinforcement Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = 2019,
        month = jan,
          eid = {arXiv:1901.08162},
        pages = {arXiv:1901.08162},
archivePrefix = {arXiv},
       eprint = {1901.08162},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190108162D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



