{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(\n",
    "    1,\n",
    "    '/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib'\n",
    ")\n",
    "\n",
    "import data_load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import dowhy.datasets\n",
    "import dowhy\n",
    "import propensity_score_estimator as pse\n",
    "import incremental_ps_score_estimator as ipse\n",
    "import math\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import utils \n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "from dowhy import CausalModel\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import sem\n",
    "from dowhy import CausalModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from econml.dml import LinearDMLCateEstimator\n",
    "from sklearn.linear_model import LassoCV\n",
    "from econml.inference import BootstrapInference\n",
    "from econml.dml import SparseLinearDMLCateEstimator\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = 't'\n",
    "outcome = 'yf'\n",
    "col =  [\"t\", \"yf\", \"ycf\", \"mu0\", \"mu1\" ]\n",
    "cov = [\"x\" + str(i) for i in range(1,26)]\n",
    "col = col + cov\n",
    "features = cov + [\"t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with Adam and original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return tf.reduce_mean(influence)\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.001])\n",
    "    delta = tf.Variable(100., trainable = True)\n",
    "    true_effect = np.mean(data['mu1'] - data['mu0'])\n",
    "    \n",
    "    for i in range(80000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.math.abs(true_effect - influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            ## early stopping\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-119b9b3d5526>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-119b9b3d5526>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    if==0:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "delta_seq = []\n",
    "losses = []\n",
    "for index_ in tqdm(range(1, 101)): \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data['p0'] = 1 - data['p1']\n",
    "\n",
    "    ## Compute counterfactual outcome with no treatment\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "    data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                          (1 - data['p1']))\n",
    "    \n",
    "    data['w0'] = data['ips_weight']*data[treatment]\n",
    "    data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
    "    \n",
    "    delta, loss = optimization(data)\n",
    "    delta_r = delta.numpy()\n",
    "    delta_seq.append(delta_r)\n",
    "    losses.append(loss.numpy())\n",
    "    print(\"Data index {}. Delta {:.2f}. Loss {:.2f}\".format(index_, delta_r, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = asarray(delta_seq)\n",
    "save('delta.npy', delta_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with list of delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return tf.reduce_mean(influence)\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.05])\n",
    "    '''\n",
    "    delta = tf.Variable(\n",
    "        tf.random.uniform([data.shape[0],], \n",
    "                          minval=1, \n",
    "                          maxval=100, \n",
    "                          dtype=tf.dtypes.float32), \n",
    "                          trainable = True)\n",
    "    '''\n",
    "    delta = tf.Variable(tf.random.normal(\n",
    "        [data.shape[0],], \n",
    "        mean=0.0, \n",
    "        stddev=1.0, \n",
    "        dtype=tf.dtypes.float32, \n",
    "        seed=1, \n",
    "        name='delta'\n",
    "    ), trainable = True)\n",
    "    \n",
    "    true_effect = np.mean(data['mu1'] - data['mu0'])\n",
    "    \n",
    "    for i in range(40000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.math.abs(true_effect - influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            ## early stopping\n",
    "            ##print(\"Loss \", loss)\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:06<10:42,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 16\n",
      "Data index 1. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2/100 [00:12<10:24,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 6\n",
      "Data index 2. Loss 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 3/100 [00:19<10:35,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 112\n",
      "Data index 3. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 4/100 [00:25<10:25,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 35\n",
      "Data index 4. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 5/100 [00:32<10:15,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 8\n",
      "Data index 5. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 6/100 [00:38<10:06,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 5\n",
      "Data index 6. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 7/100 [00:45<10:11,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 33\n",
      "Data index 7. Loss 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 8/100 [00:52<10:07,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 34\n",
      "Data index 8. Loss 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 9/100 [06:37<2:44:17, 108.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data index 9. Loss 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 10/100 [06:45<1:57:07, 78.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 74\n",
      "Data index 10. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 11/100 [06:52<1:24:18, 56.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 27\n",
      "Data index 11. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 12/100 [07:00<1:01:33, 41.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 10\n",
      "Data index 12. Loss 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 13/100 [07:07<45:48, 31.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 48\n",
      "Data index 13. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 14/100 [07:14<34:52, 24.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 26\n",
      "Data index 14. Loss 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 15/100 [07:22<27:14, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 20\n",
      "Data index 15. Loss 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 16/100 [17:37<4:37:14, 198.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data index 16. Loss 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 17/100 [17:43<3:14:12, 140.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 39\n",
      "Data index 17. Loss 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 18/100 [17:48<2:16:38, 99.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 20\n",
      "Data index 18. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 19/100 [17:54<1:36:43, 71.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 2\n",
      "Data index 19. Loss 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 20/100 [18:01<1:09:39, 52.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 58\n",
      "Data index 20. Loss 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██        | 21/100 [18:08<50:50, 38.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 42\n",
      "Data index 21. Loss 0.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 22/100 [18:14<37:45, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 44\n",
      "Data index 22. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 23/100 [18:21<28:31, 22.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 27\n",
      "Data index 23. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 24/100 [18:27<22:12, 17.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 39\n",
      "Data index 24. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 25/100 [18:39<19:42, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 28\n",
      "Data index 25. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 26/100 [26:13<3:01:34, 147.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data index 26. Loss 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 27/100 [26:19<2:07:43, 104.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 110\n",
      "Data index 27. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 28/100 [26:25<1:30:20, 75.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 62\n",
      "Data index 28. Loss 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▉       | 29/100 [26:31<1:04:27, 54.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 52\n",
      "Data index 29. Loss 0.01\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "delta_seq = []\n",
    "for index_ in tqdm(range(1, 101)): \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data['p0'] = 1 - data['p1']\n",
    "\n",
    "    ## Compute counterfactual outcome with no treatment\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "    data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                          (1 - data['p1']))\n",
    "    \n",
    "    data['w0'] = data['ips_weight']*data[treatment]\n",
    "    data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
    "    \n",
    "    delta, loss = optimization(data)\n",
    "    delta_r = delta.numpy()\n",
    "    delta_seq.append(delta_r)\n",
    "    print(\"Data index {}. Loss {:.2f}\".format(index_, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('list_delta_.npy', delta_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for individual treatment effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return influence\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.01])\n",
    "    '''\n",
    "    delta = tf.Variable(\n",
    "        tf.random.uniform([data.shape[0],], \n",
    "                          minval=1, \n",
    "                          maxval=100, \n",
    "                          dtype=tf.dtypes.float32), \n",
    "                          trainable = True)\n",
    "    '''\n",
    "    delta = tf.Variable(tf.random.normal(\n",
    "        [data.shape[0],], \n",
    "        mean=0, \n",
    "        stddev=10, \n",
    "        dtype=tf.dtypes.float32, \n",
    "        seed=1, \n",
    "        name='delta'\n",
    "    ), trainable = True)\n",
    "    \n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    \n",
    "    for i in range(80000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            #print(true_effect)\n",
    "            #print(influence)\n",
    "            loss = tf.keras.losses.MAE(true_effect, influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            if i % 5000 == 0:\n",
    "                print(\"Epoch {}. Loss {:.5f}\".format(i, loss.numpy()))\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss 175.66507\n",
      "Epoch 5000. Loss 16.96419\n",
      "Epoch 10000. Loss 11.77645\n",
      "Epoch 15000. Loss 9.76440\n",
      "Epoch 20000. Loss 8.73238\n",
      "Epoch 25000. Loss 8.09316\n",
      "Epoch 30000. Loss 7.64659\n",
      "Epoch 35000. Loss 7.30521\n",
      "Epoch 40000. Loss 7.03583\n",
      "Epoch 45000. Loss 6.82551\n",
      "Epoch 50000. Loss 6.65692\n",
      "Epoch 55000. Loss 6.51818\n",
      "Epoch 60000. Loss 6.40253\n",
      "Epoch 65000. Loss 6.30351\n",
      "Epoch 70000. Loss 6.21924\n",
      "Epoch 75000. Loss 6.14970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [10:24<00:00, 624.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data index 13. Loss 6.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "delta_seq = []\n",
    "for index_ in tqdm(range(13, 14)): \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data['p0'] = 1 - data['p1']\n",
    "\n",
    "    ## Compute counterfactual outcome with no treatment\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "    data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                          (1 - data['p1']))\n",
    "    \n",
    "    data['w0'] = data['ips_weight']*data[treatment]\n",
    "    data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
    "    \n",
    "    delta, loss = optimization(data)\n",
    "    delta_r = delta.numpy()\n",
    "    delta_seq.append(delta_r)\n",
    "    print(\"Data index {}. Loss {:.2f}\".format(index_, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('individual_list_delta_.npy', delta_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-phd_env] *",
   "language": "python",
   "name": "conda-env-.conda-phd_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
