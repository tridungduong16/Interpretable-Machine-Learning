{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(\n",
    "    1,\n",
    "    '/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib'\n",
    ")\n",
    "\n",
    "import data_load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import dowhy.datasets\n",
    "import dowhy\n",
    "import propensity_score_estimator as pse\n",
    "import incremental_ps_score_estimator as ipse\n",
    "import math\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import utils \n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "from dowhy import CausalModel\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import sem\n",
    "from dowhy import CausalModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from econml.dml import LinearDMLCateEstimator\n",
    "from sklearn.linear_model import LassoCV\n",
    "from econml.inference import BootstrapInference\n",
    "from econml.dml import SparseLinearDMLCateEstimator\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from geneticalgorithm import geneticalgorithm as ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = 't'\n",
    "outcome = 'yf'\n",
    "col =  [\"t\", \"yf\", \"ycf\", \"mu0\", \"mu1\" ]\n",
    "cov = [\"x\" + str(i) for i in range(1,26)]\n",
    "col = col + cov\n",
    "features = cov + [\"t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with Adam and original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return tf.reduce_mean(influence)\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.001])\n",
    "    delta = tf.Variable(100., trainable = True)\n",
    "    true_effect = np.mean(data['mu1'] - data['mu0'])\n",
    "    \n",
    "    for i in range(80000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.math.abs(true_effect - influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            ## early stopping\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-057649cb41e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ips_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtreatment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdelta_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdelta_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9988f9fdf64b>\u001b[0m in \u001b[0;36moptimization\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0minfluence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincre_ps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_effect\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minfluence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0md_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9988f9fdf64b>\u001b[0m in \u001b[0;36mincre_ps\u001b[0;34m(delta, data)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cf1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0minfluence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfluence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phd_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phd_env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2063\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m       gen_math_ops.mean(\n\u001b[0;32m-> 2065\u001b[0;31m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2066\u001b[0m           name=name))\n\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phd_env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis, reduction_indices)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;31m# Fast path: avoid creating Rank and Range ops if ndims is known.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1618\u001b[0;31m       \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/phd_env/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=access-member-before-definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "delta_seq = []\n",
    "losses = []\n",
    "for index_ in tqdm(range(1, 101)): \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data['p0'] = 1 - data['p1']\n",
    "\n",
    "    ## Compute counterfactual outcome with no treatment\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "    data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                          (1 - data['p1']))\n",
    "    \n",
    "    data['w0'] = data['ips_weight']*data[treatment]\n",
    "    data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
    "    \n",
    "    delta, loss = optimization(data)\n",
    "    delta_r = delta.numpy()\n",
    "    delta_seq.append(delta_r)\n",
    "    losses.append(loss.numpy())\n",
    "    print(\"Data index {}. Delta {:.2f}. Loss {:.2f}\".format(index_, delta_r, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = asarray(delta_seq)\n",
    "save('delta.npy', delta_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with list of delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return tf.reduce_mean(influence)\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.001])\n",
    "    '''\n",
    "    delta = tf.Variable(\n",
    "        tf.random.uniform([data.shape[0],], \n",
    "                          minval=1, \n",
    "                          maxval=100, \n",
    "                          dtype=tf.dtypes.float32), \n",
    "                          trainable = True)\n",
    "    '''\n",
    "    delta = tf.Variable(tf.random.normal(\n",
    "        [data.shape[0],], \n",
    "        mean=0.0, \n",
    "        stddev=1.0, \n",
    "        dtype=tf.dtypes.float32, \n",
    "        seed=1, \n",
    "        name='delta'\n",
    "    ), trainable = True)\n",
    "    \n",
    "    true_effect = np.mean(data['mu1'] - data['mu0'])\n",
    "    \n",
    "    for i in range(50000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.math.abs(true_effect - influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            ## early stopping\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [04:49<38:39, 289.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 1. Loss 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 2/9 [04:54<23:50, 204.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance reach MAE: 0.001. Cancelling the training at step 31\n",
      "(672,)\n",
      "Data index 2. Loss 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 3/9 [09:35<22:43, 227.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 3. Loss 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 4/9 [14:23<20:27, 245.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 4. Loss 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 5/9 [19:06<17:06, 256.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 5. Loss 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 6/9 [23:53<13:17, 265.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 6. Loss 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 7/9 [28:45<09:07, 273.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 7. Loss 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 8/9 [33:31<04:37, 277.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 8. Loss 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [38:27<00:00, 256.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "Data index 9. Loss 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "delta_seq = []\n",
    "for index_ in tqdm(range(1, 10)): \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data['p0'] = 1 - data['p1']\n",
    "\n",
    "    ## Compute counterfactual outcome with no treatment\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "    data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                          (1 - data['p1']))\n",
    "    \n",
    "    data['w0'] = data['ips_weight']*data[treatment]\n",
    "    data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
    "    \n",
    "    delta, loss = optimization(data)\n",
    "    delta_r = delta.numpy()\n",
    "    print(delta_r.shape)\n",
    "    delta_seq.append(delta_r)\n",
    "    print(\"Data index {}. Loss {:.2f}\".format(index_, loss.numpy()))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAAAVCAYAAADYb8kIAAAABHNCSVQICAgIfAhkiAAABJZJREFUWIXt2Hus13MYB/BXNzrLJXKd7QxDC1mYhilFaRgTMjOpodjkMpbblh0ZhSls5rJstdnKOCpRYjTJIqw2psIUSpZbuVQk+eP5fDvf8z3f3zm/00nL5b399vn+nufzeT7P5/k+t8+3XV1dnf+x49C+An0K1qLLTtTln4QTsRVXFxllBj0JQzEev+bo7TAC7+KXxHsf11aQs6NxJqbjG/yGrzEX5+TmDBcHbe63pSC3mzDMdHyGjViPBbhK+dk+wAzcgz3yjI4lk+/FT3i8QH8GlwnPnYoNGJjmnYorSmTtKDyA0ViFF/Ed9hee0g+z07wluLuCjD44A3MK9CHiDGswD1/iQFyISTg7zdlaWDdOONcNuC8jFg16FAYkQRtz9MHCmCvQOx0IdkO98OgZeKHCYdqCEcKYUzASvxf4nXLPS9KvDAvT+FSB/gnOx8v4M0e/E4twkTBufWHdIizDNSKa/6SpO18pQvvZAn1wGh/SYEzicGPS86gKB2kLdhcR86VyY8LmKuT0xMlYLQyXxxuYpbExidTyRHruV0HuNNSKSEVTDx0gcsw7BfpBafy8RGhG6yM8tuzQ24uBIrQfFgc+F8dik/CQhZWXNsLIND6taQ5tDtnL+qMC/+2cnnNpbNAu6IWlGhcjGrzysBKhh6exY3peVr2+LeKkNG7CYmHMPObjYnzbjIwaXC4MOakVe3fUUBdeqTDnvTT2zQj5kD8EHURyLiILk5uxb47eSeMisE+VylaLA9I4WhSFPtgTx+FVcZDnWpBxCboKo3zVir3Hixc4W/K+EqwXL7s2I+Q9tFsafyxZOE0UnkH4GDOToAE4WOS4Wk3zUFuRvfA/ROFYmf5/KPL6cpyOU1QO/yzcn2zFvjfgFhFtQ1uY+4PoChopTENV71yyaAvOw+0ivIal36eiZfo5zVvbCqWrwbo0LtZgzAwbNHhO7wrrjxH6rdLQWrWEUXhEOE5/YbDmUCPXEeU9NDNGN+XYjPvTL4/OOFLk2RVVqVw9lqdxXQV+Fk01FfitLUY3YSI+EheJlhykvUgnK/KEDGuE93WvYuM8LhXVfWor11WD10XuPFr5jSUrUmUvsrMI1y3CoC3hNmHMJcIzq4m27qLN3Nb75pXcKqrmfjiiZPFeJbReeFB4yvgS/uQkd3gVypXhC9Ej1uLGAu8skdPXKa/CQ0SRnKPlYjRG6P+B8Mzvmp++DSencV5GKPah9eJmMEjca/N4TeSKj0TO7CH6wo0iv35dsmG+qGwvrsPxmJD2WyzatwuE910tqm0RWbgXb0ZFDMPYJOstUZCKWCmco4iz0rqZGaHMoGtF//VYgfe8CO/LRc5anZQdJ5J+GXoK4xdvJ63BKnFnv0tU+r7iW8OstPeikjU9cJrqilHWW3cQObQMb2pq0L3FS31JLgLalXwPvUNc9k8Q3rC96IrvxXX11jbI2VVxPR4VvfGCjFiW6CeKvnJsGzfsIzqDCW2UsyuiRjhevZwxKf98t0lUx/7iOlq8hlaLWcp72n8DDhXpbnKRUWZQotrP//v0+cdjKerKGDvjS/t/Cn8BLr4Qk8k4r0QAAAAASUVORK5CYII=\n",
      "text/latex": [
       "$\\displaystyle \\left( 9, \\  672\\right)$"
      ],
      "text/plain": [
       "(9, 672)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(delta_seq).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('list_delta_.npy', delta_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Optimization for individual treatment effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return influence\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.01])\n",
    "    '''\n",
    "    delta = tf.Variable(\n",
    "        tf.random.uniform([data.shape[0],], \n",
    "                          minval=1, \n",
    "                          maxval=100, \n",
    "                          dtype=tf.dtypes.float32), \n",
    "                          trainable = True)\n",
    "    '''\n",
    "    delta = tf.Variable(tf.random.normal(\n",
    "        [data.shape[0],], \n",
    "        mean=0, \n",
    "        stddev=10, \n",
    "        dtype=tf.dtypes.float32, \n",
    "        seed=1, \n",
    "        name='delta'\n",
    "    ), trainable = True)\n",
    "    \n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    \n",
    "    for i in range(100000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.keras.losses.MAE(true_effect, influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Epoch {}. Loss {:.5f}\".format(i, loss.numpy()))\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss 128.21106\n",
      "Epoch 1000. Loss 35.25104\n",
      "Epoch 2000. Loss 26.33487\n",
      "Epoch 3000. Loss 22.03906\n",
      "Epoch 4000. Loss 19.51452\n",
      "Epoch 5000. Loss 17.79730\n",
      "Epoch 6000. Loss 16.47934\n",
      "Epoch 7000. Loss 15.40867\n",
      "Epoch 8000. Loss 14.58724\n",
      "Epoch 9000. Loss 13.90437\n",
      "Epoch 10000. Loss 13.29880\n",
      "Epoch 11000. Loss 12.75287\n",
      "Epoch 12000. Loss 12.24901\n",
      "Epoch 13000. Loss 11.83835\n",
      "Epoch 14000. Loss 11.50295\n",
      "Epoch 15000. Loss 11.18480\n",
      "Epoch 16000. Loss 10.90050\n",
      "Epoch 17000. Loss 10.66358\n",
      "Epoch 18000. Loss 10.45095\n",
      "Epoch 19000. Loss 10.25655\n",
      "Epoch 20000. Loss 10.07695\n",
      "Epoch 21000. Loss 9.91892\n",
      "Epoch 22000. Loss 9.77304\n",
      "Epoch 23000. Loss 9.63611\n",
      "Epoch 24000. Loss 9.50814\n",
      "Epoch 25000. Loss 9.38981\n",
      "Epoch 26000. Loss 9.27811\n",
      "Epoch 27000. Loss 9.17255\n",
      "Epoch 28000. Loss 9.07324\n",
      "Epoch 29000. Loss 8.97934\n",
      "Epoch 30000. Loss 8.89013\n",
      "Epoch 31000. Loss 8.80546\n",
      "Epoch 32000. Loss 8.72501\n",
      "Epoch 33000. Loss 8.64893\n",
      "Epoch 34000. Loss 8.57677\n",
      "Epoch 35000. Loss 8.50806\n",
      "Epoch 36000. Loss 8.44242\n",
      "Epoch 37000. Loss 8.37966\n",
      "Epoch 38000. Loss 8.31963\n",
      "Epoch 39000. Loss 8.26250\n",
      "Epoch 40000. Loss 8.20809\n",
      "Epoch 41000. Loss 8.15584\n",
      "Epoch 42000. Loss 8.10573\n",
      "Epoch 43000. Loss 8.05758\n",
      "Epoch 44000. Loss 8.01163\n",
      "Epoch 45000. Loss 7.96834\n",
      "Epoch 46000. Loss 7.92674\n",
      "Epoch 47000. Loss 7.88663\n",
      "Epoch 48000. Loss 7.84795\n",
      "Epoch 49000. Loss 7.81053\n",
      "Epoch 50000. Loss 7.77444\n",
      "Epoch 51000. Loss 7.73965\n",
      "Epoch 52000. Loss 7.70595\n",
      "Epoch 53000. Loss 7.67409\n",
      "Epoch 54000. Loss 7.64323\n",
      "Epoch 55000. Loss 7.61338\n",
      "Epoch 56000. Loss 7.58440\n",
      "Epoch 57000. Loss 7.55627\n",
      "Epoch 58000. Loss 7.52914\n",
      "Epoch 59000. Loss 7.50283\n",
      "Epoch 60000. Loss 7.47723\n",
      "Epoch 61000. Loss 7.45239\n",
      "Epoch 62000. Loss 7.42889\n",
      "Epoch 63000. Loss 7.40609\n",
      "Epoch 64000. Loss 7.38403\n",
      "Epoch 65000. Loss 7.36268\n",
      "Epoch 66000. Loss 7.34194\n",
      "Epoch 67000. Loss 7.32175\n",
      "Epoch 68000. Loss 7.30209\n",
      "Epoch 69000. Loss 7.28293\n",
      "Epoch 70000. Loss 7.26449\n",
      "Epoch 71000. Loss 7.24649\n",
      "Epoch 72000. Loss 7.22891\n",
      "Epoch 73000. Loss 7.21174\n",
      "Epoch 74000. Loss 7.19496\n",
      "Epoch 75000. Loss 7.17856\n",
      "Epoch 76000. Loss 7.16253\n",
      "Epoch 77000. Loss 7.14686\n",
      "Epoch 78000. Loss 7.13159\n",
      "Epoch 79000. Loss 7.11670\n",
      "Epoch 80000. Loss 7.10213\n",
      "Epoch 81000. Loss 7.08787\n",
      "Epoch 82000. Loss 7.07391\n",
      "Epoch 83000. Loss 7.06024\n",
      "Epoch 84000. Loss 7.04697\n",
      "Epoch 85000. Loss 7.03399\n",
      "Epoch 86000. Loss 7.02127\n",
      "Epoch 87000. Loss 7.00881\n",
      "Epoch 88000. Loss 6.99659\n",
      "Epoch 89000. Loss 6.98461\n",
      "Epoch 90000. Loss 6.97286\n",
      "Epoch 91000. Loss 6.96134\n",
      "Epoch 92000. Loss 6.95009\n",
      "Epoch 93000. Loss 6.93910\n",
      "Epoch 94000. Loss 6.92838\n",
      "Epoch 95000. Loss 6.91798\n",
      "Epoch 96000. Loss 6.90786\n",
      "Epoch 97000. Loss 6.89798\n",
      "Epoch 98000. Loss 6.88830\n",
      "Epoch 99000. Loss 6.87881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:38<00:00, 578.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data index 13. Loss 6.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "delta_seq = []\n",
    "for index_ in tqdm(range(13, 14)): \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data['p0'] = 1 - data['p1']\n",
    "\n",
    "    ## Compute counterfactual outcome with no treatment\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "    data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                          (1 - data['p1']))\n",
    "    \n",
    "    data['w0'] = data['ips_weight']*data[treatment]\n",
    "    data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
    "    \n",
    "    delta, loss = optimization(data)\n",
    "    delta_r = delta.numpy()\n",
    "    delta_seq.append(delta_r)\n",
    "    print(\"Data index {}. Loss {:.2f}\".format(index_, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('individual_list_delta_.npy', delta_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization for individual treatment effects using genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return influence\n",
    "\n",
    "\n",
    "def fitness_function(delta, solution_idx):\n",
    "    influence = incre_ps(delta, data)\n",
    "    mae = -np.sqrt(np.mean((true_effect - influence)**2))\n",
    "    return np.mean(mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "data = utils.load_data(PATH_TRAIN, 1)\n",
    "\n",
    "## Fit treatment\n",
    "model_t = LogisticRegression()\n",
    "model_t.fit(data[cov], data[treatment])\n",
    "\n",
    "## Fit outcome\n",
    "model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "model_y.fit(data[features], data[outcome])\n",
    "\n",
    "data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "data['p0'] = 1 - data['p1']\n",
    "\n",
    "## Compute counterfactual outcome with no treatment\n",
    "data_pos = data.copy()\n",
    "data_pos[treatment] = 1\n",
    "data['cf1'] = model_y.predict(data_pos[features])\n",
    "\n",
    "## Compute counterfactual outcome with treatment\n",
    "data_neg = data.copy()\n",
    "data_neg[treatment] = 0\n",
    "data['cf0'] = model_y.predict(data_neg[features])\n",
    "\n",
    "data['ips_weight'] = (data[treatment] / data['p1'] + (1 - data[treatment]) /\n",
    "                      (1 - data['p1']))\n",
    "\n",
    "data['w0'] = data['ips_weight']*data[treatment]\n",
    "data['w1'] = data['ips_weight']*(1 - data[treatment])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_effect = data['mu1'] - data['mu0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "import numpy\n",
    "\n",
    "\"\"\"\n",
    "Given the following function:\n",
    "    y = f(w1:w6) = w1x1 + w2x2 + w3x3 + w4x4 + w5x5 + 6wx6\n",
    "    where (x1,x2,x3,x4,x5,x6)=(4,-2,3.5,5,-11,-4.7) and y=44\n",
    "What are the best values for the 6 weights (w1 to w6)? We are going to use the genetic algorithm to optimize this function.\n",
    "\"\"\"\n",
    "\n",
    "#function_inputs = [4,-2,3.5,5,-11,-4.7] # Function inputs.\n",
    "#desired_output = 44 # Function output.\n",
    "\n",
    "function_inputs = np.random.rand(data.shape[0]) # Function inputs.\n",
    "\n",
    "num_generations = 5000 # Number of generations.\n",
    "num_parents_mating = 100 # Number of solutions to be selected as parents in the mating pool.\n",
    "\n",
    "# To prepare the initial population, there are 2 ways:\n",
    "# 1) Prepare it yourself and pass it to the initial_population parameter. This way is useful when the user wants to start the genetic algorithm with a custom initial population.\n",
    "# 2) Assign valid integer values to the sol_per_pop and num_genes parameters. If the initial_population parameter exists, then the sol_per_pop and num_genes parameters are useless.\n",
    "sol_per_pop = 2500 # Number of solutions in the population.\n",
    "num_genes = len(function_inputs)\n",
    "\n",
    "init_range_low = -100\n",
    "init_range_high = 100\n",
    "\n",
    "parent_selection_type = \"sss\" # Type of parent selection.\n",
    "keep_parents = -1 # Number of parents to keep in the next population. -1 means keep all parents and 0 means keep nothing.\n",
    "\n",
    "crossover_type = \"single_point\" # Type of the crossover operator.\n",
    "\n",
    "# Parameters of the mutation operation.\n",
    "mutation_type = \"random\" # Type of the mutation operator.\n",
    "mutation_percent_genes = 10 # Percentage of genes to mutate. This parameter has no action if the parameter mutation_num_genes exists or when mutation_type is None.\n",
    "\n",
    "last_fitness = 0\n",
    "def callback_generation(ga_instance):\n",
    "    global last_fitness\n",
    "    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n",
    "    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n",
    "    print(\"Change     = {change}\".format(change=ga_instance.best_solution()[1] - last_fitness))\n",
    "    print(\"\\n\")\n",
    "    last_fitness = ga_instance.best_solution()[1]\n",
    "\n",
    "# Creating an instance of the GA class inside the ga module. Some parameters are initialized within the constructor.\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating, \n",
    "                       fitness_func=fitness_function,\n",
    "                       sol_per_pop=sol_per_pop, \n",
    "                       num_genes=num_genes,\n",
    "                       init_range_low=init_range_low,\n",
    "                       init_range_high=init_range_high,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       keep_parents=keep_parents,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       callback_generation=callback_generation)\n",
    "\n",
    "# Running the GA to optimize the parameters of the function.\n",
    "ga_instance.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-phd_env] *",
   "language": "python",
   "name": "conda-env-.conda-phd_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
