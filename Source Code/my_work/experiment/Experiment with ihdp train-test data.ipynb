{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(\n",
    "    1,\n",
    "    '/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib'\n",
    ")\n",
    "\n",
    "import data_load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import incremental_ps_score_estimator as ipse\n",
    "import math\n",
    "import timeit\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dowhy.datasets\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "\n",
    "from math import sqrt\n",
    "from econml.drlearner import ForestDRLearner, LinearDRLearner\n",
    "from econml.metalearners import SLearner, XLearner, TLearner\n",
    "from econml.ortho_forest import CausalTree, ContinuousTreatmentOrthoForest, DiscreteTreatmentOrthoForest\n",
    "from econml.dml import ForestDMLCateEstimator, LinearDMLCateEstimator, SparseLinearDMLCateEstimator\n",
    "from econml.inference import BootstrapInference\n",
    "from econml.sklearn_extensions.linear_model import WeightedLasso, WeightedLassoCV\n",
    "\n",
    "### Import sklearn\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "from zepid.causal.doublyrobust import TMLE\n",
    "from cforest.forest import CausalForest\n",
    "from bartpy.sklearnmodel import SklearnModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = 't'\n",
    "outcome = 'yf'\n",
    "col =  [\"t\", \"yf\", \"ycf\", \"mu0\", \"mu1\" ]\n",
    "cov = [\"x\" + str(i) for i in range(1,26)]\n",
    "col = col + cov\n",
    "features = cov + [\"t\"]\n",
    "\n",
    "RESULT_PATH = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"/home/dtd/Downloads/dragonnet/data_train_test\"\n",
    "# for index_ in tqdm(range(1,101)):    \n",
    "#     data = utils.load_data(PATH_TRAIN, index_)\n",
    "#     data_test = utils.load_data(PATH_TEST, index_)\n",
    "#     data.to_csv(PATH + \"/train_ihdp_{}\",format(index_),encoding='utf-8')\n",
    "#     data_test.to_csv(PATH + \"/test_ihdp_{}\",format(index_), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with EconML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_result = pd.DataFrame(index=range(100))\n",
    "df_train_result['mae_linear_ortho'] = 0\n",
    "df_train_result['mae_linear_dr'] = 0\n",
    "df_train_result['mae_forest_dr'] = 0\n",
    "df_train_result['mae_x_leaner'] = 0\n",
    "df_train_result['mae_s_leaner'] = 0\n",
    "df_train_result['mae_t_leaner'] = 0\n",
    "df_train_result['incremental'] = 0\n",
    "df_train_result['causal_forest'] = 0\n",
    "\n",
    "\n",
    "df_test_result = pd.DataFrame(index=range(100))\n",
    "df_test_result['mae_linear_ortho'] = 0\n",
    "df_test_result['mae_linear_dr'] = 0\n",
    "df_test_result['mae_forest_dr'] = 0\n",
    "df_test_result['mae_x_leaner'] = 0\n",
    "df_test_result['mae_s_leaner'] = 0\n",
    "df_test_result['mae_t_leaner'] = 0\n",
    "df_test_result['incremental'] = 0\n",
    "df_test_result['causal_forest'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:08<00:00,  6.69s/it]\n"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "    \n",
    "    data['term_t'] = data[treatment].apply(lambda x: bool(x))\n",
    "    data_test['term_t'] = data_test[treatment].apply(lambda x: bool(x))\n",
    "    \n",
    "    cf = CausalForest(\n",
    "        num_trees=50,\n",
    "        split_ratio=0.5,\n",
    "        min_leaf=5,\n",
    "        max_depth=20,\n",
    "        use_transformed_outcomes=True,\n",
    "        num_workers=4,\n",
    "        seed_counter=1,\n",
    "    )\n",
    "\n",
    "    cf = cf.fit(data[features].values, data['term_t'].values, data[outcome].values)\n",
    "\n",
    "    ate_t_learner_train = np.mean(cf.predict(data[features].values))\n",
    "    ate_t_learner_test = np.mean(cf.predict(data_test[features].values))\n",
    "    \n",
    "    \n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means_train, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means_test, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "\n",
    "    mae_t_leaner_train = utils.abs_ate(means_train, ate_t_learner_train)\n",
    "    mae_t_leaner_test = utils.abs_ate(means_test, ate_t_learner_test)\n",
    "    \n",
    "    df_train_result.loc[index_ - 1, 'causal_forest'] = mae_t_leaner_train\n",
    "    df_test_result.loc[index_ - 1, 'causal_forest'] = mae_t_leaner_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE training 0.915\n",
      "MAE testing 1.181\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE training {:.3f}\".format(df_train_result.causal_forest.mean()))\n",
    "print(\"MAE testing {:.3f}\".format(df_test_result.causal_forest.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_bart(data, bart_model):\n",
    "    data_pos = data.copy()\n",
    "    data_pos[treatment] = 1\n",
    "    data['treated_cf_outcome'] = bart_model.predict(data_pos[features])\n",
    "\n",
    "    ## Compute counterfactual outcome with treatment\n",
    "    data_neg = data.copy()\n",
    "    data_neg[treatment] = 0\n",
    "    data['control_cf_outcome'] = bart_model.predict(data_neg[features])\n",
    "    \n",
    "    return np.mean(data['treated_cf_outcome'] - data['control_cf_outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [16:30<1:50:49, 74.71s/it]"
     ]
    }
   ],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "df_train_result['bart'] = 0\n",
    "df_test_result['bart'] = 0\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "    \n",
    "    bart_model = SklearnModel() # Use default parameters\n",
    "    bart_model.fit(data[features].values, data[outcome].values)\n",
    "    \n",
    "    effects_train = causal_bart(data, bart_model)\n",
    "    effects_test = causal_bart(data_test, bart_model)\n",
    "\n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means_train, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means_test, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_t_leaner_train = utils.abs_ate(means_train, effects_train)\n",
    "    mae_t_leaner_test = utils.abs_ate(means_test, effects_test)\n",
    "    \n",
    "    \n",
    "    df_train_result.loc[index_ - 1, 'bart'] = mae_t_leaner_train\n",
    "    df_test_result.loc[index_ - 1, 'bart'] = mae_t_leaner_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BART training {:.3f}\".format(df_train_result.causal_forest.mean()))\n",
    "print(\"BART testing {:.3f}\".format(df_test_result.causal_forest.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run EconML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T Leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "# PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "# for index_ in tqdm(range(1,101)):    \n",
    "#     data = utils.load_data(PATH_TRAIN, index_)\n",
    "#     data_test = utils.load_data(PATH_TEST, index_)\n",
    "    \n",
    "#     true_effect = data['mu1'] - data['mu0']\n",
    "#     means_train, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "#     true_effect = data_test['mu1'] - data_test['mu0']\n",
    "#     means_test, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "#     ## T-Learner\n",
    "#     est = TLearner(models=GradientBoostingRegressor())\n",
    "#     est.fit(data[outcome].values, data[treatment].values, data[features].values)\n",
    "    \n",
    "#     ate_t_learner_train = np.mean(est.effect(data[features].values))\n",
    "#     ate_t_learner_test = np.mean(est.effect(data_test[features].values))\n",
    "#     mae_t_leaner_train = utils.abs_ate(means_train, ate_t_learner_train)\n",
    "#     mae_t_leaner_test = utils.abs_ate(means_test, ate_t_learner_test)\n",
    "#     df_train_result.loc[index_ - 1, 'mae_t_leaner'] = round(mae_t_leaner_train,2)\n",
    "#     df_test_result.loc[index_ - 1, 'mae_t_leaner'] = round(mae_t_leaner_test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_result.mae_t_leaner.mean(), df_test_result.mae_t_leaner.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Double ML\n",
    "    est = LinearDMLCateEstimator(model_y=LassoCV(), model_t=LassoCV())\n",
    "    ### Estimate with OLS confidence intervals\n",
    "    est.fit(Y = data[outcome].values, T = data[treatment].values, X = data[features].values, inference='statsmodels')\n",
    "    ate_dr_train = np.mean(est.effect(data[features].values))\n",
    "    print(ate_dr_train)\n",
    "\n",
    "    est = SparseLinearDMLCateEstimator(model_y=LassoCV(), model_t=LassoCV())\n",
    "    ### Estimate with OLS confidence intervals\n",
    "    est.fit(Y = data[outcome].values, T = data[treatment].values, X = data[features].values, inference='statsmodels')\n",
    "    ate_dr_train = np.mean(est.effect(data[features].values))\n",
    "    print(ate_dr_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Linear\n",
    "    est = LinearDRLearner(model_propensity=GradientBoostingClassifier(),\n",
    "                          model_regression=GradientBoostingRegressor())\n",
    "    est.fit(Y = data[outcome].values, T = data[treatment].values, X = data[features].values, inference='statsmodels')\n",
    "    ate_dr_train = np.mean(est.effect(data[features].values))\n",
    "    ate_dr_test = np.mean(est.effect(data_test[features].values))\n",
    "    \n",
    "    ## Forest\n",
    "    est = ForestDRLearner(model_propensity=GradientBoostingClassifier(),\n",
    "                      model_regression=GradientBoostingRegressor())\n",
    "    est.fit(Y = data[outcome].values, T = data[treatment].values, X = data[features].values, inference='blb')\n",
    "    ate_fdr_train = np.mean(est.effect(data[features].values))\n",
    "    ate_fdr_test = np.mean(est.effect(data_test[features].values))\n",
    "    \n",
    "    \"\"\"\n",
    "    ## X Learner\n",
    "    est = XLearner(models=GradientBoostingRegressor(),\n",
    "              propensity_model=GradientBoostingClassifier(),\n",
    "              cate_models=GradientBoostingRegressor())\n",
    "    est.fit(data[outcome].values, data[treatment].values, data[features].values)\n",
    "\n",
    "    ate_x_learner_train = np.mean(est.effect(data[features].values))\n",
    "    ate_x_learner_test = np.mean(est.effect(data_test[features].values))\n",
    "    \n",
    "    ## S-Learner\n",
    "    est = SLearner(overall_model=GradientBoostingRegressor())\n",
    "    est.fit(data[outcome].values, data[treatment].values, data[features].values)\n",
    "    \n",
    "    ate_s_learner_train = np.mean(est.effect(data[features].values))\n",
    "    ate_s_learner_test = np.mean(est.effect(data_test[features].values))\n",
    "    \n",
    "    \n",
    "    ## T-Learner\n",
    "    \n",
    "    \n",
    "    est = TLearner(models=GradientBoostingRegressor())\n",
    "    est = TLearner(models=LassoCV())\n",
    "    est.fit(data[outcome].values, data[treatment].values, data[features].values)\n",
    "    \n",
    "    ate_t_learner_train = np.mean(est.effect(data[features].values))\n",
    "    ate_t_learner_test = np.mean(est.effect(data_test[features].values))\n",
    "    \n",
    "    \"\"\"\n",
    "    ## OrthoForest\n",
    "    est = DiscreteTreatmentOrthoForest(n_trees=500,\n",
    "                                   model_Y = WeightedLasso(alpha=0.5))\n",
    "\n",
    "    est.fit(Y = data[outcome].values, T = data[treatment].values, X = data[features].values, inference='blb')\n",
    "    \n",
    "    ate_ortho_train = np.mean(est.effect(data[features].values))\n",
    "    ate_ortho_test = np.mean(est.effect(data_test[features].values))\n",
    "    \n",
    "    ### df_train\n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_linear_ortho = utils.abs_ate(means, ate_ortho_train)\n",
    "    mae_linear_dr = utils.abs_ate(means, ate_dr_train)\n",
    "    mae_forest_dr = utils.abs_ate(means, ate_fdr_train)\n",
    "    #mae_x_leaner = utils.abs_ate(means, ate_x_learner_train)\n",
    "    #mae_s_leaner = utils.abs_ate(means, ate_s_learner_train)\n",
    "    #mae_t_leaner = utils.abs_ate(means, ate_t_learner_train)\n",
    "    \n",
    "\n",
    "    df_train_result.loc[index_ - 1, 'mae_linear_ortho'] = round(mae_linear_ortho,2)\n",
    "    df_train_result.loc[index_ - 1, 'mae_linear_dr'] = round(mae_linear_dr,2)\n",
    "    df_train_result.loc[index_ - 1, 'mae_forest_dr'] = round(mae_forest_dr,2)\n",
    "    #df_train_result.loc[index_ - 1, 'mae_x_leaner'] = round(mae_x_leaner,2)\n",
    "    #df_train_result.loc[index_ - 1, 'mae_s_leaner'] = round(mae_s_leaner,2)\n",
    "    #df_train_result.loc[index_ - 1, 'mae_t_leaner'] = round(mae_t_leaner,2)\n",
    "\n",
    "    \n",
    "\n",
    "    ## df_test\n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_linear_ortho = utils.abs_ate(means, ate_ortho_test)\n",
    "    mae_linear_dr = utils.abs_ate(means, ate_dr_test)\n",
    "    mae_forest_dr = utils.abs_ate(means, ate_fdr_test)\n",
    "    #mae_x_leaner = utils.abs_ate(means, ate_x_learner_test)\n",
    "    #mae_s_leaner = utils.abs_ate(means, ate_s_learner_test)\n",
    "    #mae_t_leaner = utils.abs_ate(means, ate_t_learner_test)\n",
    "    \n",
    "    df_test_result.loc[index_ - 1, 'mae_linear_ortho'] = round(mae_linear_ortho,2)\n",
    "    df_test_result.loc[index_ - 1, 'mae_linear_dr'] = round(mae_linear_dr,2)\n",
    "    df_test_result.loc[index_ - 1, 'mae_forest_dr'] = round(mae_forest_dr,2)\n",
    "    #df_test_result.loc[index_ - 1, 'mae_x_leaner'] = round(mae_x_leaner,2)\n",
    "    #df_test_result.loc[index_ - 1, 'mae_s_leaner'] = round(mae_s_leaner,2)\n",
    "    #df_test_result.loc[index_ - 1, 'mae_t_leaner'] = round(mae_t_leaner,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbor + Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "df_train_result['mea_knn'] = 0\n",
    "df_test_result['mea_knn'] = 0\n",
    "\n",
    "df_train_result['linear_regression'] = 0\n",
    "df_test_result['linear_regression'] = 0\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "    \n",
    "    model = CausalModel(data=data,\n",
    "                        treatment=treatment,\n",
    "                        outcome=outcome,\n",
    "                        common_causes=cov, proceed_when_unidentifiable=True)\n",
    "    data[treatment] = [bool(x) for x in data[treatment]]\n",
    "    identified_estimand = model.identify_effect()\n",
    "\n",
    "    estimate_we = model.estimate_effect(\n",
    "            identified_estimand, method_name=\"backdoor.propensity_score_matching\")\n",
    "\n",
    "    estimate_linear = model.estimate_effect(\n",
    "            identified_estimand, method_name=\"backdoor.linear_regression\")\n",
    "    \n",
    "    \n",
    "    model = CausalModel(data=data_test,\n",
    "                        treatment=treatment,\n",
    "                        outcome=outcome,\n",
    "                        common_causes=cov, proceed_when_unidentifiable=True)\n",
    "    data_test[treatment] = [bool(x) for x in data_test[treatment]]\n",
    "    identified_estimand = model.identify_effect()\n",
    "\n",
    "    estimate_we_test = model.estimate_effect(\n",
    "        identified_estimand, method_name=\"backdoor.propensity_score_matching\")\n",
    "    \n",
    "    estimate_linear_test = model.estimate_effect(\n",
    "            identified_estimand, method_name=\"backdoor.linear_regression\")\n",
    "    \n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means_test, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "        \n",
    "    mae_incre_train = utils.abs_ate(means, estimate_we.value)\n",
    "    mae_incre_test = utils.abs_ate(means_test, estimate_we_test.value)\n",
    "    df_train_result.loc[index_ - 1, 'mea_knn'] = mae_incre_train\n",
    "    df_test_result.loc[index_ - 1, 'mea_knn'] = mae_incre_test\n",
    "    \n",
    "    mae_incre_train = utils.abs_ate(means, estimate_linear.value)\n",
    "    mae_incre_test = utils.abs_ate(means_test, estimate_linear_test.value)\n",
    "    df_train_result.loc[index_ - 1, 'linear_regression'] = mae_incre_train\n",
    "    df_test_result.loc[index_ - 1, 'linear_regression'] = mae_incre_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_result.mea_knn.mean())\n",
    "print(df_test_result.mea_knn.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "expose_m = ' + '.join(cov)\n",
    "outcome_m = ' + '.join(features)\n",
    "\n",
    "df_train_result['mea_tmle'] = 0\n",
    "df_test_result['mea_tmle'] = 0\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "    model_t = LogisticRegression(penalty='l2', random_state=201)\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    \n",
    "    tml = TMLE(data, exposure=treatment, outcome=outcome)\n",
    "    tml.exposure_model(expose_m, bound=[0.01, 0.99], print_results=False, custom_model=model_t)\n",
    "    tml.outcome_model(outcome_m, print_results=False, custom_model = model_y)\n",
    "    tml.fit()\n",
    "    \n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    mae_train = utils.abs_ate(means, tml.average_treatment_effect)\n",
    "\n",
    "    tml = TMLE(data_test, exposure=treatment, outcome=outcome)\n",
    "    tml.exposure_model(expose_m, bound=[0.01, 0.99], print_results=False, custom_model=model_t)\n",
    "    tml.outcome_model(outcome_m, print_results=False)\n",
    "    tml.fit()\n",
    "    \n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    mae_test = utils.abs_ate(means, tml.average_treatment_effect)\n",
    "        \n",
    "    df_train_result.loc[index_ - 1, 'mea_tmle'] = mae_train\n",
    "    df_test_result.loc[index_ - 1, 'mea_tmle'] = mae_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function for base models\n",
    "def rmse(yreal, yhat):\n",
    "    return sqrt(mean_squared_error(yreal, yhat))\n",
    "def get_super_learner_outcome(X):\n",
    "    ensemble = SuperLearner(scorer=rmse, folds=10, shuffle=True, sample_size=len(X))\n",
    "    \n",
    "    models = list()\n",
    "    models.append(LinearRegression())\n",
    "    models.append(ElasticNet())\n",
    "    models.append(SVR(gamma='scale'))\n",
    "    models.append(DecisionTreeRegressor())\n",
    "    models.append(GradientBoostingRegressor(random_state=0, n_estimators = 5000))\n",
    "    models.append(AdaBoostRegressor())\n",
    "    models.append(BaggingRegressor(n_estimators=100))\n",
    "    models.append(RandomForestRegressor(n_estimators=100))\n",
    "    models.append(ExtraTreesRegressor(n_estimators=100))\n",
    "    ensemble.add(models)\n",
    "    ensemble.add_meta(LinearRegression())\n",
    "    return ensemble\n",
    "\n",
    "def f1(y, p): return f1_score(y, p, average='micro')\n",
    "\n",
    "\n",
    "def get_super_learner_treatment(X):\n",
    "    ensemble = SuperLearner(scorer=f1, random_state=1)\n",
    "    \n",
    "    models = list()\n",
    "    models.append(KNeighborsClassifier(3))\n",
    "    models.append(RandomForestClassifier(n_estimators=30, random_state=1))\n",
    "    models.append(GradientBoostingClassifier())\n",
    "    models.append(SVC())\n",
    "    models.append(AdaBoostClassifier())\n",
    "    models.append(MLPClassifier(alpha=1, max_iter=1000))\n",
    "    models.append(GaussianProcessClassifier(1.0 * RBF(1.0)))\n",
    "    \n",
    "\n",
    "    ensemble.add(models)\n",
    "    ensemble.add_meta(LogisticRegression())\n",
    "    return ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta_seq = np.load('delta.npy')\n",
    "\n",
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "mae_incre_seq  = []\n",
    "rmse  = []\n",
    "rmse1  = []\n",
    "rmse2  = []\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "\n",
    "\n",
    "    #Incremental propensity score\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    #model_t = get_super_learner_treatment(data)\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "    \n",
    "\n",
    "    ## Fit outcome\n",
    "    \n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features].values, data[outcome].values)\n",
    "    y_pred = model_y.predict(data_test[features].values)\n",
    "    rmse.append(mean_squared_error(data_test[outcome].values, \n",
    "                                       y_pred,\n",
    "                                       sample_weight=None, \n",
    "                                       multioutput='uniform_average', \n",
    "                                       squared=False))\n",
    "\n",
    "    \n",
    "    \n",
    "    delta = delta_seq[index_ - 1]\n",
    "    \n",
    "    ## df_train\n",
    "    influence = ipse.influence_function(data, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "    means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "    print(delta, np.mean(influence))\n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_incre = utils.abs_ate(means, means_incre)\n",
    "    \n",
    "    df_train_result.loc[index_ - 1, 'incremental'] = mae_incre\n",
    "    print(\"MAE {:.2f}\".format(mae_incre))\n",
    "    \n",
    "    ## df_test\n",
    "    influence = ipse.influence_function(data_test, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "    means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_incre = utils.abs_ate(means, means_incre)\n",
    "    \n",
    "    df_test_result.loc[index_ - 1, 'incremental'] = mae_incre\n",
    "    \n",
    "    print(\"MAE {:.2f}\".format(mae_incre))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE - Incremental \", np.mean(df_train_result.incremental.mean()))\n",
    "print(\"MAE - Incremental \", np.mean(df_test_result.incremental.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE - Incremental  0.42532114248173786\n",
    "# MAE - Incremental  0.5544695165630058\n",
    "## Gradient boosting\n",
    "# MAE - Incremental  0.2173673527217913\n",
    "# MAE - Incremental  0.6196024525840684\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfrnet = (0.299  + 0.262)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_seq = np.load('list_delta.npy')\n",
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "\n",
    "for index_ in tqdm(range(1,101)):    \n",
    "    data = utils.load_data(PATH_TRAIN, index_)\n",
    "    data_test = utils.load_data(PATH_TEST, index_)\n",
    "\n",
    "    #Incremental propensity score\n",
    "    ## Fit treatment\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(data[cov], data[treatment])\n",
    "    \n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(data[features], data[outcome])\n",
    "\n",
    "    data['p1'] = model_t.predict_proba(data[cov])[:,1]\n",
    "    data_test['p1'] = model_t.predict_proba(data_test[cov])[:,1]\n",
    "\n",
    "    treated_neighbors = (\n",
    "            NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "            .fit(data['p1'].values.reshape(-1, 1))\n",
    "    )\n",
    "    distances, indices = treated_neighbors.kneighbors(data_test['p1'].values.reshape(-1, 1))\n",
    "    \n",
    "    #print(data.loc[578,'p1'], data_test.loc[0,'p1'])\n",
    "    #print(data.loc[440,'p1'], data_test.loc[1,'p1'])\n",
    "    #print(data.loc[143,'p1'], data_test.loc[2,'p1'])\n",
    "    #print(data.loc[564,'p1'], data_test.loc[3,'p1'])\n",
    "\n",
    "    delta = delta_seq[index_ - 1]\n",
    "    \n",
    "    ## df_train\n",
    "    influence = ipse.influence_function(data, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "    means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_incre = utils.abs_ate(means, means_incre)\n",
    "    \n",
    "    df_train_result.loc[index_ - 1, 'incremental_con'] = mae_incre\n",
    "    \n",
    "    ## df_test\n",
    "    delta = delta_seq[index_ - 1][indices.reshape(-1)]\n",
    "    influence = ipse.influence_function(data_test, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "    means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "    true_effect = data_test['mu1'] - data_test['mu0']\n",
    "    means, stds = np.mean(true_effect, axis=0), sem(true_effect, axis=0)\n",
    "    \n",
    "    mae_incre = utils.abs_ate(means, means_incre)\n",
    "    \n",
    "    df_test_result.loc[index_ - 1, 'incremental_con'] = mae_incre\n",
    "    \n",
    "    print(\"Data index {}. MAE {:.2f}\".format(index_, mae_incre))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_train = 0.282\n",
    "mmd_test = 0.299\n",
    "wass_train  = 0.255\n",
    "wass_test  = 0.262\n",
    "bnn_0_4_train = (1.445 + 2.021) / 2\n",
    "bnn_0_4_test = (1.240 + 1.791) / 2\n",
    "bnn_2_2_train = 0\n",
    "bnn_2_2_test = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = {}\n",
    "training_result[\"Linear_Regression\"] = df_train_result.linear_regression.mean()\n",
    "training_result[\"K-NN\"] = df_train_result.mea_knn.mean()\n",
    "training_result[\"Incremental_Con_PS\"] = df_train_result.incremental_con.mean()\n",
    "training_result[\"CFR_WASS\"] = wass_train\n",
    "training_result[\"CFR_MMD\"] = mmd_train\n",
    "training_result[\"Incremental_PS\"]= df_train_result.incremental.mean()\n",
    "training_result[\"SLearner\"] = df_train_result.mae_s_leaner.mean()\n",
    "training_result[\"OrthoForest\"] = df_train_result.mae_linear_ortho.mean()\n",
    "training_result[\"ForestDRLearner\"] = df_train_result.mae_forest_dr.mean()\n",
    "training_result[\"LinearDRLearner\"] = df_train_result.mae_linear_ortho.mean()\n",
    "training_result[\"Bart\"] = df_train_result.bart.mean()\n",
    "training_result[\"Causal_forest\"] = df_train_result.causal_forest.mean()\n",
    "         \n",
    "test_result = {}\n",
    "test_result[\"Linear_Regression\"] = df_test_result.linear_regression.mean()\n",
    "test_result[\"K-NN\"] = df_test_result.mea_knn.mean()\n",
    "test_result[\"Incremental_Con_PS\"] = df_test_result.incremental_con.mean()\n",
    "test_result[\"CFR_WASS\"] = wass_train\n",
    "test_result[\"CFR_MMD\"] = mmd_train\n",
    "test_result[\"Incremental_PS\"]= df_test_result.incremental.mean()\n",
    "test_result[\"SLearner\"] = df_test_result.mae_s_leaner.mean()\n",
    "test_result[\"OrthoForest\"] = df_test_result.mae_linear_ortho.mean()\n",
    "test_result[\"ForestDRLearner\"] = df_test_result.mae_forest_dr.mean()\n",
    "test_result[\"LinearDRLearner\"] = df_test_result.mae_linear_ortho.mean()\n",
    "test_result[\"Bart\"] = df_test_result.bart.mean()\n",
    "test_result[\"Causal_forest\"] = df_test_result.causal_forest.mean()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_result = {k: v for k, v in sorted(training_result.items(), key=lambda item: item[1])}\n",
    "test_result = {k: v for k, v in sorted(test_result.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set\")\n",
    "for k,v in training_result.items():\n",
    "    print(\"\\t\", k, \"{:.3f}\".format(v))\n",
    "    \n",
    "print(\"Testing set\")\n",
    "for k,v in test_result.items():\n",
    "    print(\"\\t\", k, \"{:.3f}\".format(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RESULT = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/result/idhp\"\n",
    "df_train_result.to_csv(PATH_RESULT + \"/train_result.csv\", index = False)\n",
    "df_test_result.to_csv(PATH_RESULT + \"/test_result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.train.npz\"\n",
    "PATH_TEST = \"/home/dtd/Documents/interpretable_machine_learning/Source Code/data/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "df_train = utils.load_data(PATH_TRAIN, 1)\n",
    "df_test = utils.load_data(PATH_TEST, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_one_hot_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "_one_hot_encoder.fit(df_train.t.values.reshape((-1,1)))\n",
    "t = _one_hot_encoder.transform(df_train.t.values.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = LogisticRegression()\n",
    "model_t.fit(df_train[cov], t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-phd_env] *",
   "language": "python",
   "name": "conda-env-.conda-phd_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.883px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
