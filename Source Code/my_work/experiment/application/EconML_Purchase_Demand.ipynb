{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(\n",
    "    1,\n",
    "    '/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib'\n",
    ")\n",
    "\n",
    "import data_load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import incremental_ps_score_estimator as ipse\n",
    "import math\n",
    "import timeit\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import dowhy.datasets\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from econml.drlearner import ForestDRLearner, LinearDRLearner\n",
    "from econml.metalearners import SLearner, XLearner, TLearner\n",
    "from econml.ortho_forest import CausalTree, ContinuousTreatmentOrthoForest, DiscreteTreatmentOrthoForest\n",
    "from econml.dml import ForestDMLCateEstimator, LinearDMLCateEstimator, SparseLinearDMLCateEstimator\n",
    "from econml.inference import BootstrapInference\n",
    "from econml.sklearn_extensions.linear_model import WeightedLasso, WeightedLassoCV\n",
    "\n",
    "### Import sklearn\n",
    "from scipy.stats import sem\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from cforest.forest import CausalForest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"https://msalicedatapublic.blob.core.windows.net/datasets/Pricing/pricing_sample.csv\"\n",
    "train_data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    4346\n",
       "0.8    3089\n",
       "0.9    2565\n",
       "Name: price, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.price.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['treatment'] = np.where(train_data['price'] == 1, 1, 0)\n",
    "train_data['price'] = np.where(train_data['price'] == 1, 1, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85    5654\n",
       "1.00    4346\n",
       "Name: price, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.price.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_age</th>\n",
       "      <th>age</th>\n",
       "      <th>avg_hours</th>\n",
       "      <th>days_visited</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>has_membership</th>\n",
       "      <th>is_US</th>\n",
       "      <th>songs_purchased</th>\n",
       "      <th>income</th>\n",
       "      <th>price</th>\n",
       "      <th>demand</th>\n",
       "      <th>treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>1.834234</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.903237</td>\n",
       "      <td>0.960863</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.917117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>7.171411</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.330161</td>\n",
       "      <td>0.732487</td>\n",
       "      <td>1.00</td>\n",
       "      <td>11.585706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>5.351920</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.036203</td>\n",
       "      <td>1.130937</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.675960</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>6.723551</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.911926</td>\n",
       "      <td>0.929197</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.361776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>2.448247</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.148967</td>\n",
       "      <td>0.533527</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12.624123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_age  age  avg_hours  days_visited  friends_count  has_membership  \\\n",
       "0            3   53   1.834234             2              8               1   \n",
       "1            5   54   7.171411             7              9               0   \n",
       "2            3   33   5.351920             6              9               0   \n",
       "3            2   34   6.723551             0              8               0   \n",
       "4            4   30   2.448247             5              8               1   \n",
       "\n",
       "   is_US  songs_purchased    income  price     demand  treatment  \n",
       "0      1         4.903237  0.960863   1.00   3.917117          1  \n",
       "1      1         3.330161  0.732487   1.00  11.585706          1  \n",
       "2      1         3.036203  1.130937   1.00  24.675960          1  \n",
       "3      1         7.911926  0.929197   1.00   6.361776          1  \n",
       "4      0         7.148967  0.533527   0.85  12.624123          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account_age', 'age', 'avg_hours', 'days_visited', 'friends_count', 'has_membership', 'is_US', 'songs_purchased', 'income', 'demand', 'treatment']\n",
      "['account_age', 'age', 'avg_hours', 'days_visited', 'friends_count', 'has_membership', 'is_US', 'songs_purchased']\n",
      "['account_age', 'age', 'avg_hours', 'days_visited', 'friends_count', 'has_membership', 'is_US', 'songs_purchased', 'income', 'treatment']\n"
     ]
    }
   ],
   "source": [
    "outcome = \"demand\"\n",
    "treatment = \"treatment\"\n",
    "col = list(train_data.columns)\n",
    "col.remove(\"price\")\n",
    "print(col)\n",
    "\n",
    "cov = col[:]\n",
    "cov.remove(treatment)\n",
    "cov.remove(outcome)\n",
    "cov.remove('income')\n",
    "print(cov)\n",
    "\n",
    "features = col[:]\n",
    "features.remove(outcome)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5654\n",
       "1    4346\n",
       "Name: treatment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.treatment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "X_test = np.linspace(0, 5, 100).reshape(-1, 1)\n",
    "X_test_data = pd.DataFrame(X_test, columns=[\"income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define underlying treatment effect function given DGP\n",
    "def gamma_fn(X):\n",
    "    return -3 - 14 * (X[\"income\"] < 1)\n",
    "\n",
    "def beta_fn(X):\n",
    "    return 20 + 0.5 * (X[\"avg_hours\"]) + 5 * (X[\"days_visited\"] > 4)\n",
    "\n",
    "def demand_fn(data, T):\n",
    "    Y = gamma_fn(data) * T + beta_fn(data)\n",
    "    return Y\n",
    "\n",
    "def true_te(x, n, stats):\n",
    "    if x < 1:\n",
    "        subdata = train_data[train_data[\"income\"] < 1].sample(n=n, replace=True)\n",
    "    else:\n",
    "        subdata = train_data[train_data[\"income\"] >= 1].sample(n=n, replace=True)\n",
    "    te_array = subdata[\"price\"] * gamma_fn(subdata) / (subdata[\"demand\"])\n",
    "    if stats == \"mean\":\n",
    "        return np.mean(te_array)\n",
    "    elif stats == \"median\":\n",
    "        return np.median(te_array)\n",
    "    elif isinstance(stats, int):\n",
    "        return np.percentile(te_array, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the estimate and range of true treatment effect\n",
    "truth_te_estimate = np.apply_along_axis(true_te, 1, X_test, 1000, \"mean\")  # estimate\n",
    "truth_te_upper = np.apply_along_axis(true_te, 1, X_test, 1000, 95)  # upper level\n",
    "truth_te_lower = np.apply_along_axis(true_te, 1, X_test, 1000, 5)  # lower level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation true effect -0.45369095264802645\n",
      "Upper bound true effect -0.2795216649663177\n",
      "Lower bound true effect -0.75770637353686\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimation true effect {}\".format(np.mean(truth_te_estimate)))\n",
    "print(\"Upper bound true effect {}\".format(np.mean(truth_te_upper)))\n",
    "print(\"Lower bound true effect {}\".format(np.mean(truth_te_lower)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_array = train_data[\"price\"] * gamma_fn(train_data) / (train_data[\"demand\"])\n",
    "true_effect = np.mean(te_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation effect with incremental propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account_age',\n",
       " 'age',\n",
       " 'avg_hours',\n",
       " 'days_visited',\n",
       " 'friends_count',\n",
       " 'has_membership',\n",
       " 'is_US',\n",
       " 'songs_purchased',\n",
       " 'income',\n",
       " 'treatment']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['account_age',\n",
       " 'age',\n",
       " 'avg_hours',\n",
       " 'days_visited',\n",
       " 'friends_count',\n",
       " 'has_membership',\n",
       " 'is_US',\n",
       " 'songs_purchased',\n",
       " 'treatment']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cov.copy()\n",
    "x.append(treatment)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of train data 0.5654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(n_estimators=5000, random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit treatment\n",
    "model_t = LogisticRegression()\n",
    "model_t.fit(train_data[cov], train_data[treatment])\n",
    "\n",
    "train_data['p1'] = model_t.predict_proba(train_data[cov])[:,1]\n",
    "train_data['p0'] = 1 - train_data['p1']\n",
    "\n",
    "train_data[\"prediction\"] = np.where(train_data[\"p1\"] >= 0.5, 1, 0)\n",
    "acc = accuracy_score(train_data[\"prediction\"], train_data[\"treatment\"])\n",
    "print(\"Accuracy score of train data {}\".format(acc))\n",
    "\n",
    "## Fit outcome\n",
    "model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "model_y.fit(train_data[features], train_data[outcome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation effect -0.94\n",
      "MAE 0.01\n"
     ]
    }
   ],
   "source": [
    "delta = -0.015843108\n",
    "influence = ipse.influence_function(train_data, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "print(\"Estimation effect {:.2f}\".format(means_incre))\n",
    "print(\"MAE {:.2f}\".format(utils.abs_ate(true_effect, means_incre)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation effect in train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/dtd/.conda/envs/phd_env/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of train data 0.5644444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(n_estimators=5000, random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(train_data, test_size=0.1, random_state = 1)\n",
    "\n",
    "## Fit treatment\n",
    "model_t = LogisticRegression()\n",
    "model_t.fit(train[cov], train[treatment])\n",
    "\n",
    "train['p1'] = model_t.predict_proba(train[cov])[:,1]\n",
    "train['p0'] = 1 - train['p1']\n",
    "\n",
    "train[\"prediction\"] = np.where(train[\"p1\"] >= 0.5, 1, 0)\n",
    "acc = accuracy_score(train[\"prediction\"], train[\"treatment\"])\n",
    "print(\"Accuracy score of train data {}\".format(acc))\n",
    "\n",
    "## Fit outcome\n",
    "model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "model_y.fit(train[features], train[outcome])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['p1'] = model_t.predict_proba(data[covariate])[:,1]\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['p0'] = 1 - data['p1']\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cf1'] = model_y.predict(data_pos[features])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cf0'] = model_y.predict(data_neg[features])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['q1'] = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['q1'] = data['q1'].abs()\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['q0'] = 1 - data['q1']\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  (1 - data['p1']))\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['w0'] = data['ips_weight']*data[treatment]\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['a0'] = data['q0']*data['w0']*(data['cf0'] - data[outcome])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['a1'] = data['q1']*data['w1']*(data['cf1'] - data[outcome])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['p1'] = model_t.predict_proba(data[covariate])[:,1]\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['p0'] = 1 - data['p1']\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cf1'] = model_y.predict(data_pos[features])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cf0'] = model_y.predict(data_neg[features])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['q1'] = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['q1'] = data['q1'].abs()\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['q0'] = 1 - data['q1']\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  (1 - data['p1']))\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['w0'] = data['ips_weight']*data[treatment]\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['w1'] = data['ips_weight']*(1 - data[treatment])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['a0'] = data['q0']*data['w0']*(data['cf0'] - data[outcome])\n",
      "/home/dtd/Documents/interpretable_machine_learning/Source Code/my_work/lib/incremental_ps_score_estimator.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['a1'] = data['q1']*data['w1']*(data['cf1'] - data[outcome])\n"
     ]
    }
   ],
   "source": [
    "delta = 2.0\n",
    "\n",
    "te_array = test[\"price\"] * gamma_fn(test) / (test[\"demand\"])\n",
    "true_effect_test = np.mean(te_array)\n",
    "\n",
    "te_array = train[\"price\"] * gamma_fn(train) / (train[\"demand\"])\n",
    "true_effect_train = np.mean(te_array)\n",
    "\n",
    "influence = ipse.influence_function(train, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "means_incre_train, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "influence = ipse.influence_function(test, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "means_incre_test, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "origin_mae_train = utils.abs_ate(true_effect_train, means_incre_train)\n",
    "origin_mae_test = utils.abs_ate(true_effect_test, means_incre_test)\n",
    "\n",
    "# print(\"Estimation effect {:.2f}\".format(means_incre))\n",
    "# print(\"MAE {:.2f}\".format(utils.abs_ate(true_effect, means_incre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7068891358439671 0.739408703015396\n"
     ]
    }
   ],
   "source": [
    "print(origin_mae_train, origin_mae_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with single delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return tf.reduce_mean(influence), influence\n",
    "\n",
    "def optimization(data, true_effect):\n",
    "    threhold = tf.constant([0.001])\n",
    "    delta = tf.Variable(np.random.randint(low=1, high=40, size=1), \n",
    "                        trainable = True, \n",
    "                        dtype = tf.float32)\n",
    "    delta_seq = []\n",
    "    losses = []\n",
    "    effects = []\n",
    "    influences = []\n",
    "    for i in tqdm(range(100000)):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu_influence, influence = incre_ps(delta, data)\n",
    "            loss = tf.math.abs(true_effect - mu_influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            ## early stopping \n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "            delta_seq.append(delta.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "            effects.append(mu_influence.numpy())\n",
    "            influences.append(influence.numpy())\n",
    "        if i % 3000 == 0:\n",
    "            print(\"Epoch {}. Loss {:.4f}\".format(i, loss))\n",
    "    print(\"Loss {:.3f}\".format(loss))\n",
    "    print(\"Effects \", influence)\n",
    "    return delta, delta_seq, losses, effects, influences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/100000 [00:00<11:47, 141.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss 1.1236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3022/100000 [00:18<10:47, 149.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3000. Loss 1.1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6016/100000 [00:38<10:59, 142.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6000. Loss 1.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9023/100000 [00:58<10:44, 141.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9000. Loss 1.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12019/100000 [01:18<08:52, 165.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12000. Loss 1.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15021/100000 [01:37<10:25, 135.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15000. Loss 1.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18021/100000 [01:57<11:24, 119.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18000. Loss 1.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21028/100000 [02:18<08:41, 151.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21000. Loss 0.9329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24028/100000 [02:37<07:52, 160.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24000. Loss 0.7084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 25980/100000 [02:49<08:45, 140.73it/s]"
     ]
    }
   ],
   "source": [
    "te_array = train[\"price\"] * gamma_fn(train) / (train[\"demand\"])\n",
    "true_effect = np.mean(te_array)\n",
    "\n",
    "delta, delta_seq, losses, effects, influences = optimization(train, true_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_seq, effects, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_seq, losses, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_seq[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_re = delta.numpy()\n",
    "delta_re = delta_seq[-1]\n",
    "\n",
    "delta_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_array = test[\"price\"] * gamma_fn(test) / (test[\"demand\"])\n",
    "true_effect_test = np.mean(te_array)\n",
    "\n",
    "te_array = train[\"price\"] * gamma_fn(train) / (train[\"demand\"])\n",
    "true_effect_train = np.mean(te_array)\n",
    "\n",
    "influence_train = ipse.influence_function(train, treatment, cov, outcome, features, delta_re, model_y, model_t)\n",
    "means_incre_train, stds_incre = np.mean(influence_train, axis=0), sem(influence, axis=0)\n",
    "\n",
    "influence_test = ipse.influence_function(test, treatment, cov, outcome, features, delta_re, model_y, model_t)\n",
    "means_incre_test, stds_incre = np.mean(influence_test, axis=0), sem(influence, axis=0)\n",
    "\n",
    "optimal_mae_train = utils.abs_ate(true_effect_train, means_incre_train)\n",
    "optimal_mae_test = utils.abs_ate(true_effect_test, means_incre_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before Optimization\")\n",
    "print(\"MAE on training {} and testing {}\".format(origin_mae_train, origin_mae_test))\n",
    "print(\"After Optimization\")\n",
    "print(\"MAE on training {} and testing {}\".format(optimal_mae_train, optimal_mae_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowers_train, uppers_train = [], []\n",
    "lowers_test, uppers_test = [], []\n",
    "means_train, means_test = [], []\n",
    "\n",
    "for i in tqdm(range(len(delta_seq))):\n",
    "    train_expr = train.copy()\n",
    "    test_expr = test.copy()\n",
    "    influence_train = influences[i]\n",
    "    delta = delta_seq[i]\n",
    "    influence_test = ipse.influence_function(test_expr, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "\n",
    "    mean_train, stds_incre_train = np.mean(influence_train, axis=0), sem(influence_train, axis=0)\n",
    "    mean_test, stds_incre_test = np.mean(influence_test, axis=0), sem(influence, axis=0)\n",
    "    \n",
    "    lower_train, upper_train = st.t.interval(0.90, \n",
    "                                         len(influence_train)-1, \n",
    "                                         loc=mean_train, \n",
    "                                         scale=stds_incre_train)\n",
    "    lower_test, upper_test = st.t.interval(0.90, \n",
    "                                             len(influence_test)-1, \n",
    "                                             loc=mean_test, \n",
    "                                             scale=stds_incre_test)\n",
    "    \n",
    "\n",
    "    lowers_train.append(lower_train)\n",
    "    uppers_train.append(upper_train)\n",
    "    lowers_test.append(lower_test)\n",
    "    uppers_test.append(upper_test)\n",
    "    means_train.append(mean_train)\n",
    "    means_test.append(mean_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_seq, means_train, label='Estimation effect')\n",
    "plt.axhline(y=true_effect_train, color='r', linestyle='-', label = \"True effect\")\n",
    "plt.fill_between(delta_seq, lowers_train, uppers_train, label=\"90% BLB CI\", alpha=0.3)\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"delta\")\n",
    "plt.title(\"The changes of effects depending on delta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(delta_seq, means_test, label='Estimation effect')\n",
    "plt.axhline(y=true_effect_test, color='r', linestyle='-', label = \"True effect\")\n",
    "plt.fill_between(delta_seq, lowers_test, uppers_test, label=\"90% BLB CI\", alpha=0.3)\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"delta\")\n",
    "plt.title(\"The changes of effects depending on delta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
    "mae_seq_train = []\n",
    "mae_seq_test = []\n",
    "\n",
    "for train_index, test_index in tqdm(cv.split(train_data)):\n",
    "    df_train, df_test = train_data.loc[train_index, :], train_data.loc[test_index, :]\n",
    "\n",
    "    model_t = LogisticRegression()\n",
    "    model_t.fit(train[cov], train[treatment])\n",
    "\n",
    "    train['p1'] = model_t.predict_proba(train[cov])[:,1]\n",
    "    train['p0'] = 1 - train['p1']\n",
    "\n",
    "    train[\"prediction\"] = np.where(train[\"p1\"] >= 0.5, 1, 0)\n",
    "    acc = accuracy_score(train[\"prediction\"], train[\"treatment\"])\n",
    "\n",
    "    ## Fit outcome\n",
    "    model_y = GradientBoostingRegressor(random_state=0, n_estimators = 5000)\n",
    "    model_y.fit(train[features], train[outcome])\n",
    "\n",
    "    delta = 0.025128545\n",
    "\n",
    "    te_array = test[\"price\"] * gamma_fn(test) / (test[\"demand\"])\n",
    "    true_effect_test = np.mean(te_array)\n",
    "\n",
    "    te_array = train[\"price\"] * gamma_fn(train) / (train[\"demand\"])\n",
    "    true_effect_train = np.mean(te_array)\n",
    "\n",
    "    influence = ipse.influence_function(train, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "    means_incre_train, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "    influence = ipse.influence_function(test, treatment, cov, outcome, features, delta, model_y, model_t)\n",
    "    means_incre_test, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "    origin_mae_train = utils.abs_ate(true_effect_train, means_incre_train)\n",
    "    origin_mae_test = utils.abs_ate(true_effect_test, means_incre_test)\n",
    "    \n",
    "    mae_seq_train.append(origin_mae_train)\n",
    "    mae_seq_test.append(origin_mae_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_train, stds_train = np.mean(mae_seq_train, axis=0), sem(mae_seq_train, axis=0)\n",
    "means_test, stds_test = np.mean(mae_seq_test, axis=0), sem(mae_seq_test, axis=0)\n",
    "\n",
    "print(\"Training {} +- {}\".format(means_train, stds_train))\n",
    "print(\"Testing {} +- {}\".format(means_test, stds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization individual treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return influence\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.01])\n",
    "    '''\n",
    "    delta = tf.Variable(\n",
    "        tf.random.uniform([data.shape[0],], \n",
    "                          minval=1, \n",
    "                          maxval=100, \n",
    "                          dtype=tf.dtypes.float32), \n",
    "                          trainable = True)\n",
    "    '''\n",
    "    delta = tf.Variable(tf.random.normal(\n",
    "        [data.shape[0],], \n",
    "        mean=10, \n",
    "        stddev=10, \n",
    "        dtype=tf.dtypes.float32, \n",
    "        seed=1, \n",
    "        name='delta'\n",
    "    ), trainable = True)\n",
    "    \n",
    "    true_effect = data['mu1'] - data['mu0']\n",
    "    \n",
    "    for i in range(50000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.keras.losses.MSE(true_effect, influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "            print(loss)\n",
    "            if tf.math.less(loss, threhold):\n",
    "                print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "                break\n",
    "    return delta, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_array = train[\"price\"] * gamma_fn(train) / (train[\"demand\"])\n",
    "true_effect = np.mean(te_array)\n",
    "\n",
    "delta, loss = optimization(train, true_effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with delta list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incre_ps(delta, data):\n",
    "    q1 = (delta * data['p1']) / (delta * data['p1'] + data['p0'])\n",
    "    q1 = tf.math.abs(q1)\n",
    "    a0 = (1-q1)*data['w0']*(data['cf0'] - data[outcome])\n",
    "    a1 = q1*data['w1']*(data['cf1'] - data[outcome])    \n",
    "    influence = a1 - a0\n",
    "    return tf.reduce_mean(influence)\n",
    "\n",
    "def optimization(data):\n",
    "    threhold = tf.constant([0.001])\n",
    "    delta = tf.Variable(\n",
    "        tf.random.uniform([data.shape[0],], \n",
    "                          minval=0, \n",
    "                          maxval=3, \n",
    "                          dtype=tf.dtypes.float32), \n",
    "                          trainable = True)\n",
    "    \n",
    "    for i in range(50000):\n",
    "        with tf.GradientTape() as tape:\n",
    "            influence = incre_ps(delta, data)\n",
    "            loss = tf.math.abs(true_effect - influence)\n",
    "            d_delta = tape.gradient(loss, delta)\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "            opt.apply_gradients(zip([d_delta], [delta]))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Epoch: {}. Loss: {:.4f}\".format(i, loss))\n",
    "        if tf.math.less(loss, threhold):\n",
    "            print(\"The performance reach MAE: 0.001. Cancelling the training at step {}\".format(i))\n",
    "            break\n",
    "            \n",
    "        \"\"\"\n",
    "        policy = np.where(data['q1'] >= 0.5, 1, 0)\n",
    "        rev1 = np.mean(revenue_fn(data = data, \n",
    "                          discount_level1 = 0, \n",
    "                          discount_level2 = 0.1, \n",
    "                          baseline_T = 1,\n",
    "                          policy = policy))\n",
    "        print(\"Revenue {:.2f}\".format(rev1))\n",
    "        \"\"\"\n",
    "    return delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = optimization(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = delta.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['p1'] = model_t.predict_proba(train[cov])[:,1]\n",
    "test['p1'] = model_t.predict_proba(test[cov])[:,1]\n",
    "\n",
    "te_array = test[\"price\"] * gamma_fn(test) / (test[\"demand\"])\n",
    "true_effect = np.mean(te_array)\n",
    "\n",
    "treated_neighbors = (\n",
    "        NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "        .fit(train['p1'].values.reshape(-1, 1))\n",
    ")\n",
    "distances, indices = treated_neighbors.kneighbors(test['p1'].values.reshape(-1, 1))\n",
    "delta_r = delta[indices.reshape(-1)]\n",
    "influence = ipse.influence_function(test, treatment, cov, outcome, features, delta_r, model_y, model_t)\n",
    "means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "mae_incre = utils.abs_ate(true_effect, means_incre)\n",
    "mae_incre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_re = delta.numpy()\n",
    "influence = ipse.influence_function(train_data, treatment, cov, outcome, features, delta_re, model_y, model_t)\n",
    "means_incre, stds_incre = np.mean(influence, axis=0), sem(influence, axis=0)\n",
    "\n",
    "print(\"Estimation effect {:.2f}\".format(means_incre))\n",
    "print(\"MAE {:.2f}\".format(utils.abs_ate(true_effect, means_incre)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-phd_env] *",
   "language": "python",
   "name": "conda-env-.conda-phd_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
